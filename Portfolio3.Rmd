---
title: "Portfolio3"
author: "Ida Elmose Brøcker"
date: "2022-11-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Assignment 3 - Machine learning

# The assignment

The Machine Learning assignment has 3 main parts: First we create a skeptical and an informed simulation, based on the meta-analysis. Second we build and test our machine learning pipeline on the simulated data. Second we apply the pipeline to the empirical data.

The report for the exam, thus, consists of the answer to all the following prompts:
- Describe your machine learning pipeline. Produce a diagram of it to guide the reader (e.g. see Rybner et al 2022 Vocal markers of autism: Assessing the generalizability of ML models), and describe the different parts: data budgeting, data preprocessing, model choice and training, assessment of performance.
- Briefly justify and describe your use of simulated data, and results from the pipeline on them.
- Describe results from applying the ML pipeline to the empirical data and what can we learn from them.

Remember: plots are very very important to communicate your process and results.

## Part I - Simulating data

Use the meta-analysis reported in Parola et al (2020), create a simulated dataset with 100 matched pairs of schizophrenia and controls, each participant producing 10 repeated measures (10 trials with their speech recorded). for each of these "recordings" (data points) produce 10 acoustic measures: 6 from the meta-analysis, 4 with just random noise. Do the same for a baseline dataset including only 10 noise variables. Tip: see the slides for the code. 


Slides:
Schizophrenia mean: 0.21 | Control mean: -0.21
TrialSD = 0.5 (the average variation between trials)
Error = 0.2 (measurement error)
Schizophrenia <- rnorm(1, rnorm(1, 0.21, 0.5), 0.2) # repeat 10 times
Control <-rnorm(1, rnorm(1,-0.21, 0.5), 0.2) # repeat 10 tim


```{r}
pacman::p_load(
  brms, 
  ggplot2, 
  tidyverse, 
  dplyr, 
  tidybayes
  )

```

```{r}
##simulate

###population size
n <- 100
trials <- 10

###effect sizes, 6 from m-a, 4 random noice
InformedEffectMean <- c(0.25, -0.55, -0.75, -1.26, 0.05, 1.89, 0, 0, 0, 0)
SkepticEffectMean <- rep(0, 10)

####individual variability across trials and measurement error
IndividualSD <- 1
TrialSD <- 0.5
Error <- 0.2

```


```{r}

###identify true effect size for each variable
for (i in seq(10)) {
  temp_informed <- tibble(
    ID = seq(n),
    TrueEffect = rnorm(n, InformedEffectMean[i], IndividualSD),
    Variable = paste0("v", i))
  temp_skeptic <- tibble(
    ID = seq(n), 
    TrueEffect = rnorm(n, SkepticEffectMean[i], IndividualSD),
        Variable = paste0("v", i))
  if (i==1) {
    d_informed_true <- temp_informed
    d_skeptic_true <- temp_skeptic
  } else {
    d_informed_true <- rbind(d_informed_true, temp_informed)
    d_skeptic_true <- rbind(d_skeptic_true, temp_skeptic)
  }
}

```

```{r}
####create tibble with one row per trial

d_trial <- tibble(expand_grid(ID = seq(n), Trial = seq(trials), Group = c("Schizophrenia", "Control")))

d_informed <- merge(d_informed_true, d_trial)
d_skeptic <- merge(d_skeptic_true, d_trial)

for (i in seq(nrow(d_informed))) {
  d_informed$measurement[i] <- ifelse(d_informed$Group[i] == "Schizophrenia",
                                rnorm(1, rnorm(1, d_informed$TrueEffect[i]/2, TrialSD), Error), 
                                 rnorm(1, rnorm(1, -d_informed$TrueEffect[i]/2, TrialSD), Error))
  d_skeptic$measurement[i] <- ifelse(d_skeptic$Group[i] == "Schizophrenia",
                                rnorm(1, rnorm(1, d_skeptic$TrueEffect[i]/2, TrialSD), Error), 
                                 rnorm(1, rnorm(1, -d_skeptic$TrueEffect[i]/2, TrialSD), Error))
}

```

```{r}
d_informed$Variable[d_informed$Variable =="v1"] <- "Pitch_mode"
d_informed$Variable[d_informed$Variable =="v2"] <- "Pitch_vari"
d_informed$Variable[d_informed$Variable =="v3"] <- "Speech_rate"
d_informed$Variable[d_informed$Variable =="v4"] <- "Spoken_time"
d_informed$Variable[d_informed$Variable =="v5"] <- "Pause_number"
d_informed$Variable[d_informed$Variable =="v6"] <- "Pause_length"
d_informed$Variable[d_informed$Variable =="v7"] <- "noise1"
d_informed$Variable[d_informed$Variable =="v8"] <- "noise2"
d_informed$Variable[d_informed$Variable =="v9"] <- "noise3"
d_informed$Variable[d_informed$Variable =="v10"] <- "noise4"

d_skeptic$Variable[d_skeptic$Variable =="v1"] <- "Pitch_mode"
d_skeptic$Variable[d_skeptic$Variable =="v2"] <- "Pitch_vari"
d_skeptic$Variable[d_skeptic$Variable =="v3"] <- "Speech_rate"
d_skeptic$Variable[d_skeptic$Variable =="v4"] <- "Spoken_time"
d_skeptic$Variable[d_skeptic$Variable =="v5"] <- "Pause_number"
d_skeptic$Variable[d_skeptic$Variable =="v6"] <- "Pause_length"
d_skeptic$Variable[d_skeptic$Variable =="v7"] <- "noise1"
d_skeptic$Variable[d_skeptic$Variable =="v8"] <- "noise2"
d_skeptic$Variable[d_skeptic$Variable =="v9"] <- "noise3"
d_skeptic$Variable[d_skeptic$Variable =="v10"] <- "noise4"


```


```{r}

#transfrom dataframe to wide format 

d_informed_wide <- d_informed %>% 
  mutate(TrueEffect = NULL) %>% 
  pivot_wider(names_from = Variable, values_from = measurement)

d_skeptic_wide <- d_skeptic %>% 
   mutate(TrueEffect = NULL) %>% 
  pivot_wider(names_from = Variable, values_from = measurement)

d_skeptic_wide

```

```{r}
#rename 

#d_informed_wide <- d_informed_wide %>%  dplyr::rename(
 # Pitch_mode = v1, Pitch_vari = v2, Speech_rate = v3, Spoken_time = v4, Pause_number = v5, Pause_length = v6, noise1 = v7, noise2 = v8, noise3 = v9, noise4 = v10
 # )

#d_skeptic_wide <- d_skeptic_wide %>% dplyr::rename(
#  Pitch_mode = v1, Pitch_vari = v2, Speech_rate = v3, Spoken_time = v4, Pause_number = #v5, Pause_length = v6, noise1 = v7, noise2 = v8, noise3 = v9, noise4 = v10
 # )


```

```{r}
#### Visualization


ggplot(d_informed, aes(measurement, fill=Group)) +
  geom_density(alpha=0.4) +
  facet_wrap(~Variable, ncol=5) + 
  theme_minimal() +
  ggtitle("Informed dataset") +
  scale_fill_brewer(palette = "Dark2")


ggplot(d_skeptic, aes(measurement, fill=Group)) +
  geom_density(alpha=0.4) +
  facet_wrap(~Variable, ncol=5) + 
  theme_minimal() +
  ggtitle("Skeptic dataset") +
  scale_fill_brewer(palette = "Dark2")



```




## Part II - ML pipeline on simulated data

On the two simulated datasets (separately) build a machine learning pipeline: i) create a data budget (e.g. balanced training and test sets); ii) pre-process the data (e.g. scaling the features); iii) fit and assess a classification algorithm on the training data (e.g. Bayesian multilevel logistic regression); iv) assess performance on the test set; v) discuss whether performance is as expected and feature importance is as expected.

Bonus question: replace the bayesian multilevel regression with a different algorithm, e.g. SVM or random forest (but really, anything you'd like to try).

```{r}

### 1) data budgeting - make a balanced training and test sets

TestID <- sample(seq(n), 20)

train_informed <- d_informed_wide %>% 
  subset(!(ID %in% TestID))

test_informed <- d_informed_wide %>% 
  subset(ID %in% TestID)


train_skeptic <- d_skeptic_wide %>% 
  subset(!(ID %in% TestID))

test_skeptic <- d_skeptic_wide %>% 
  subset(ID %in% TestID)

ls(train_informed )
```

```{r}
# 2) preprocessing - scaling the training data 

pacman::p_load(tidymodels)


#informed
rec_informed <- train_informed %>% 
  recipe(Group ~ .) %>% 
  step_scale("Pitch_mode", "Pitch_vari", "Speech_rate", "Spoken_time", "Pause_number", "Pause_length", "noise1", "noise2", "noise3", "noise4") %>% 
  step_center("Pitch_mode", "Pitch_vari", "Speech_rate", "Spoken_time", "Pause_number", "Pause_length", "noise1", "noise2", "noise3", "noise4") %>% 
  prep(training = train_informed, retain = TRUE)

train_informed_s <- juice(rec_informed)
test_informed_s <- bake(rec_informed, new_data = test_informed)


#skeptic
rec_skeptic <- train_skeptic %>% 
  recipe(Group ~ .) %>% 
  step_scale("Pitch_mode", "Pitch_vari", "Speech_rate", "Spoken_time", "Pause_number", "Pause_length", "noise1", "noise2", "noise3", "noise4") %>% 
  step_center("Pitch_mode", "Pitch_vari", "Speech_rate", "Spoken_time", "Pause_number", "Pause_length", "noise1", "noise2", "noise3", "noise4") %>% 
  prep(training = train_skeptic, retain = TRUE)

train_skeptic_s <- juice(rec_skeptic)
test_skeptic_s <- bake(rec_skeptic, new_data = test_skeptic)


train_informed
```





```{r}
### 3) fit and assess a classification algorithm on the training data (e.g. Bayesian multilevel logistic regression)

#define model
#fixed effect
PitchRange_f0 <-  bf(Group ~ 1 + Pitch_mode + Pitch_vari + Speech_rate + Spoken_time + Pause_number + Pause_length + noise1 + noise2 + noise3 + noise4)

#varying intercepts
PitchRange_f1 <- bf(Group ~ 1 + Pitch_mode + Pitch_vari + Speech_rate + Spoken_time + Pause_number + Pause_length + noise1 + noise2 + noise3 + noise4 + (1|ID))

#varying slopes
PitchRange_f2 <- bf(Group ~ 1 + Pitch_mode + Pitch_vari + Speech_rate + Spoken_time + Pause_number + Pause_length + noise1 + noise2 + noise3 + noise4 + (1 + Pitch_mode + Pitch_vari + Speech_rate + Spoken_time + Pause_number + Pause_length + noise1 + noise2 + noise3 + noise4 | ID))

#get prior
get_prior(PitchRange_f0, train_informed_s, family = bernoulli)

get_prior(PitchRange_f1, train_informed_s, family = bernoulli)

get_prior(PitchRange_f2, train_informed_s, family = bernoulli)

```

```{r}
#make priors

# simple model
PitchRange_p0 <- c(
   prior(normal(0,1), class = Intercept), 
   prior(normal(0, 1), class = b)
)

# varying intercepts model
 PitchRange_p1 <- c(
   prior(normal(0,1), class = Intercept), 
   prior(normal(0,1), class = sd),
   prior(normal(0, 1), class = b)
 )
 
 # slopes
  PitchRange_p2 <- c(
   prior(normal(0,1), class = Intercept), 
   prior(normal(0,1), class = sd),
   prior(normal(0, 1), class = b), 
   prior(lkj(1), class= "cor")
  )
  


PitchRange_prior0 <- 
  brm(
    PitchRange_f0, 
    data = train_informed_s,
    family = bernoulli,
    prior = PitchRange_p0,  
    sample_prior = "only", 
    iter = 2000,
    warmup = 500,
    backend = "cmdstanr",
    threads = threading(2),
    cores = 2,
    chains = 2,
    control = list(adapt_delta = 0.9, max_treedepth = 20))


PitchRange_prior1 <- 
  brm(
    PitchRange_f1, 
    data = train_informed_s,
    family = bernoulli,
    prior = PitchRange_p1,  
    sample_prior = "only", 
    iter = 2000,
    warmup = 500,
    backend = "cmdstanr",
    threads = threading(2),
    cores = 2,
    chains = 2,
    control = list(adapt_delta = 0.9, max_treedepth = 20))


PitchRange_prior2 <- 
  brm(
    PitchRange_f2, 
    data = train_informed_s,
    family = bernoulli,
    prior = PitchRange_p2,  
    sample_prior = "only", 
    iter = 2000,
    warmup = 500,
    backend = "cmdstanr",
    threads = threading(2),
    cores = 2,
    chains = 2,
    control = list(adapt_delta = 0.9, max_treedepth = 20))



p0 <- pp_check(PitchRange_prior0, ndraws = 100) +ggtitle('Model 0 Prior-predictive check') + xlim(0, 2)

p1 <- pp_check(PitchRange_prior1, ndraws = 100) +ggtitle('Model 1 Prior-predictive check') + xlim(0, 2)

p2 <- pp_check(PitchRange_prior2, ndraws = 100) +ggtitle('Model 2 Prior-predictive check') + xlim(0, 2)

pacman::p_load("gridExtra")
grid.arrange(p2,p1,p0,  nrow=2,
             top="Prior-predictive check")

```

```{r}
#fit the model
# simple
PitchRange_m0 <- brm(
  PitchRange_f0, 
  train_informed_s, 
  family = bernoulli, 
  prior = PitchRange_p0, 
  sample_prior = T,
  backend = "cmdstanr", 
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta = 0.9, 
                 max_treedepth = 20)
  )


#random
PitchRange_m1 <- brm(
  PitchRange_f1, 
  train_informed_s, 
  family = bernoulli, 
  prior = PitchRange_p1, 
  sample_prior = T,
  backend = "cmdstanr", 
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta = 0.9, 
                 max_treedepth = 20)
  )

#lasr
PitchRange_m2 <- brm(
  PitchRange_f2, 
  train_informed_s, 
  family = bernoulli, 
  prior = PitchRange_p2, 
  sample_prior = T,
  backend = "cmdstanr", 
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta = 0.9, 
                 max_treedepth = 20)
  )


#pp-check
p2 <- pp_check(PitchRange_m0, ndraws = 100)+ ggtitle('Model 0 Posterior-predictive check') + xlim(0, 2)

p3 <- pp_check(PitchRange_m1, ndraws = 100)+ ggtitle('Model 1 Posterior-predictive check') + xlim(0, 2)

p4 <- pp_check(PitchRange_m2, ndraws = 100)+ ggtitle('Model 2 Posterior-predictive check') + xlim(0, 2)



pacman::p_load("gridExtra")
grid.arrange(p2,p3,p4,  nrow=2,
             top="Informed Data")


```

```{r}
#skeptic

#fit the model
# simple
PitchRange_m0_skeptic <- brm(
  PitchRange_f0, 
  train_skeptic_s, 
  family = bernoulli, 
  prior = PitchRange_p0, 
  sample_prior = T,
  backend = "cmdstanr", 
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta = 0.9, 
                 max_treedepth = 20)
)


#random
PitchRange_m1_skeptic <- brm(
  PitchRange_f1, 
  train_skeptic_s, 
  family = bernoulli, 
  prior = PitchRange_p1, 
  sample_prior = T,
  backend = "cmdstanr", 
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta = 0.9, 
                 max_treedepth = 20)
  )

#lasr
PitchRange_m2_skeptic <- brm(
  PitchRange_f2, 
  train_skeptic_s, 
  family = bernoulli, 
  prior = PitchRange_p2, 
  sample_prior = T,
  backend = "cmdstanr", 
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta = 0.9, 
                 max_treedepth = 20)
  )


#pp-check
p2 <- pp_check(PitchRange_m0_skeptic, ndraws = 100)+ ggtitle('Model 0 Posterior-predictive check, skeptic') + xlim(0, 2)

p3 <- pp_check(PitchRange_m1_skeptic, ndraws = 100)+ ggtitle('Model 1 Posterior-predictive check, skeptic') + xlim(0, 2)

p4 <- pp_check(PitchRange_m2_skeptic, ndraws = 100)+ ggtitle('Model 2 Posterior-predictive check, skeptic') + xlim(0, 2)

p2
p3
p4

pacman::p_load("gridExtra")
grid.arrange(p2,p3,p4,  nrow=2,
             top="Skeptic Data")

```

```{r}

#Update plot 
#1. model
posterior0 <- as_draws_df(PitchRange_m0)
posterior1 <- as_draws_df(PitchRange_m0_skeptic)

glimpse(posterior1)

# Posterior for the informed
p11 <- ggplot(posterior0) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
  labs(title = "Posterior intercept, informed") +
  theme_minimal()


p22 <- ggplot(posterior0) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(b_Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(b_Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(b_Spoken_time), fill="purple", alpha=0.3) +
  geom_density(aes(b_Pause_number), fill="pink", alpha=0.3) +
  geom_density(aes(b_noise1), fill="green", alpha=0.3) +
  geom_density(aes(b_noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(b_noise3), fill="black", alpha=0.3) +
  geom_density(aes(b_noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior b, informed, model 0") +
  theme_minimal() 

# Posterior for the skeptic
p33 <- ggplot(posterior1) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
   labs(title = "Posterior intercept, skeptic") +
  theme_minimal()


p44 <- ggplot(posterior1) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(b_Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(b_Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(b_Spoken_time), fill="purple", alpha=0.3) +
  geom_density(aes(b_Pause_number), fill="pink", alpha=0.3) +
  geom_density(aes(b_noise1), fill="green", alpha=0.3) +
  geom_density(aes(b_noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(b_noise3), fill="black", alpha=0.3) +
  geom_density(aes(b_noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior b, skeptic, model 0") +
  theme_minimal()


grid.arrange(p11,p22,p33, p44,  nrow=2,
             top="Update plots, Fixed effects model")
```

```{r}


#Update plot 
#2. model
posterior_m1 <- as_draws_df(PitchRange_m1)
posterior_m1s <- as_draws_df(PitchRange_m1_skeptic)

glimpse(posterior0)

# Posterior for the informed
p111 <- ggplot(posterior_m1) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
  labs(title = "Posterior intercept, informed") +
  theme_minimal()

p222 <- ggplot(posterior_m1) +
  geom_density(aes(sd_ID__Intercept), fill="blue", alpha=0.3) +
  geom_density(aes(prior_sd_ID), fill="red", alpha=0.3) +
  labs(title = "Posterior SD, informed") +
  theme_minimal()

p333 <- ggplot(posterior1) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(b_Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(b_Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(b_Spoken_time), fill="purple", alpha=0.3) +
  geom_density(aes(b_Pause_number), fill="pink", alpha=0.3) +
  geom_density(aes(b_noise1), fill="green", alpha=0.3) +
  geom_density(aes(b_noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(b_noise3), fill="black", alpha=0.3) +
  geom_density(aes(b_noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior b, informed") +
  theme_minimal()

# Posterior for the skeptic
p444 <- ggplot(posterior_m1s) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
   labs(title = "Posterior b, skeptic") +
  theme_minimal()

p555 <- ggplot(posterior_m1s) +
  geom_density(aes(sd_ID__Intercept), fill="blue", alpha=0.3) +
  geom_density(aes(prior_sd_ID), fill="red", alpha=0.3) +
  labs(title = "Posterior SD, skeptic") +
  theme_minimal()

p666 <- ggplot(posterior_m1s) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(b_Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(b_Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(b_Spoken_time), fill="purple", alpha=0.3) +
  geom_density(aes(b_Pause_number), fill="pink", alpha=0.3) +
  geom_density(aes(b_noise1), fill="green", alpha=0.3) +
  geom_density(aes(b_noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(b_noise3), fill="black", alpha=0.3) +
  geom_density(aes(b_noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior b, skeptic") +
  theme_minimal()


grid.arrange(p111,p222,p333, p444, p555, p666,  nrow=2,
             top="Update plots, Random intercepts model")
```


```{r}


#Update plot 
#3. model
posterior_m2 <- as_draws_df(PitchRange_m2)
posterior_m2s <- as_draws_df(PitchRange_m2_skeptic)

glimpse(posterior_m2s)

# Posterior for the informed
p1111 <- ggplot(posterior_m2) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
  labs(title = "Posterior intercept, informed")
  theme_minimal()

p2222 <- ggplot(posterior_m2) +
  geom_density(aes(sd_ID__Intercept), fill="blue", alpha=0.3) +
  geom_density(aes(prior_sd_ID), fill="red", alpha=0.3) +
  labs(title = "Posterior SD, informed") +
  theme_minimal()


#ggplot(posterior_m2) +
 # geom_density(aes(cor_ID__Intercept), fill="blue", alpha=0.3) +
#  geom_density(aes(prior_cor_ID), fill="red", alpha=0.3) +
 # labs(title = "Posterior correlation, informed") +
#  theme_minimal()

p3333 <- ggplot(posterior_m2) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(b_Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(b_Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(b_Spoken_time), fill="purple", alpha=0.3) +
  geom_density(aes(b_Pause_number), fill="pink", alpha=0.3) +
  geom_density(aes(b_noise1), fill="green", alpha=0.3) +
  geom_density(aes(b_noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(b_noise3), fill="black", alpha=0.3) +
  geom_density(aes(b_noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior b, informed") +
  theme_minimal()

# Posterior for the skeptic
p4444 <- ggplot(posterior_m2s) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
   labs(title = "Posterior b, skeptic") +
  theme_minimal()

p5555 <- ggplot(posterior_m2s) +
  geom_density(aes(sd_ID__Intercept), fill="blue", alpha=0.3) +
  geom_density(aes(prior_sd_ID), fill="red", alpha=0.3) +
  labs(title = "Posterior SD, skeptic") +
  theme_minimal()

#ggplot(posterior_m2s) +
#  geom_density(aes(cor_ID__Intercept), fill="blue", alpha=0.3) +
#  geom_density(aes(prior_cor_ID), fill="red", alpha=0.3) +
#  labs(title = "Posterior correlation, skeptic") +
#  theme_minimal()

p6666 <- ggplot(posterior_m2s) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(b_Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(b_Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(b_Spoken_time), fill="purple", alpha=0.3) +
  geom_density(aes(b_Pause_number), fill="pink", alpha=0.3) +
  geom_density(aes(b_noise1), fill="green", alpha=0.3) +
  geom_density(aes(b_noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(b_noise3), fill="black", alpha=0.3) +
  geom_density(aes(b_noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior b, skeptic") +
  theme_minimal()

grid.arrange(p1111,p2222,p3333, p4444, p5555, p6666,  nrow=2,
             top="Update plots, Random slopes model")
```

```{r}
#interpreting the model

summary(PitchRange_m0)

summary(PitchRange_m1)

summary(PitchRange_m2)



```

```{r}
#compare model


#loo compare 
loo_m0 <- add_criterion(PitchRange_m0, criterion = "loo")
loo_m1 <- add_criterion(PitchRange_m1, criterion = "loo")
loo_m2 <- add_criterion(PitchRange_m2, criterion = "loo")

loo_compare(loo_m1, loo_m0, loo_m2)


loo_model_weights(loo_m0, loo_m1, loo_m2)
```


```{r}
# generate average predictions 
### informed
##for the trained dataset
#fixed effect
train_informed_s$PredictionsPerc0 <- predict(PitchRange_m0)[, 1]
train_informed_s$Predictions0[train_informed_s$PredictionsPerc0 > 0.5] <- "Schizophrenia"
train_informed_s$Predictions0[train_informed_s$PredictionsPerc0 <= 0.5] <- "Control"

#vary intercept
train_informed_s$PredictionsPerc1 <- predict(PitchRange_m1)[, 1]
train_informed_s$Predictions1[train_informed_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
train_informed_s$Predictions1[train_informed_s$PredictionsPerc1 <= 0.5] <- "Control"

#vary slopes
train_informed_s$PredictionsPerc2 <- predict(PitchRange_m2)[, 1]
train_informed_s$Predictions2[train_informed_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
train_informed_s$Predictions2[train_informed_s$PredictionsPerc2 <= 0.5] <- "Control"


train_informed_s <- train_informed_s %>% 
  mutate(
    Group = as.factor(Group), 
    Predictions0 = as.factor(Predictions0),
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2)
  )

train_informed_s

```

```{r}
##for the test dataset
#fixed effect
test_informed_s$PredictionsPerc0 <- predict(PitchRange_m0, newdata = test_informed_s, allow_new_levels = T)[, 1]
test_informed_s$Predictions0[test_informed_s$PredictionsPerc0 > 0.5] <- "Schizophrenia"
test_informed_s$Predictions0[test_informed_s$PredictionsPerc0 <= 0.5] <- "Control"

#vary intercept
test_informed_s$PredictionsPerc1 <- predict(PitchRange_m1, newdata = test_informed_s, allow_new_levels = T)[, 1]
test_informed_s$Predictions1[test_informed_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
test_informed_s$Predictions1[test_informed_s$PredictionsPerc1 <= 0.5] <- "Control"

#vary slopes
test_informed_s$PredictionsPerc2 <- predict(PitchRange_m2, newdata = test_informed_s, allow_new_levels = T)[, 1]
test_informed_s$Predictions2[test_informed_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
test_informed_s$Predictions2[test_informed_s$PredictionsPerc2 <= 0.5] <- "Control"


test_informed_s <- test_informed_s %>% 
  mutate(
    Group = as.factor(Group), 
    Predictions0 = as.factor(Predictions0),
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2)
  )

test_informed_s

```

```{r}
# generate average predictions 
### skeptic
##for the trained dataset
#fixed effect
train_skeptic_s$PredictionsPerc0 <- predict(PitchRange_m0)[, 1]
train_skeptic_s$Predictions0[train_skeptic_s$PredictionsPerc0 > 0.5] <- "Schizophrenia"
train_skeptic_s$Predictions0[train_skeptic_s$PredictionsPerc0 <= 0.5] <- "Control"

#vary intercept
train_skeptic_s$PredictionsPerc1 <- predict(PitchRange_m1)[, 1]
train_skeptic_s$Predictions1[train_skeptic_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
train_skeptic_s$Predictions1[train_skeptic_s$PredictionsPerc1 <= 0.5] <- "Control"

#vary slopes
train_skeptic_s$PredictionsPerc2 <- predict(PitchRange_m2)[, 1]
train_skeptic_s$Predictions2[train_skeptic_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
train_skeptic_s$Predictions2[train_skeptic_s$PredictionsPerc2 <= 0.5] <- "Control"


train_skeptic_s <- train_skeptic_s %>% 
  mutate(
    Group = as.factor(Group), 
    Predictions0 = as.factor(Predictions0),
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2)
  )

train_skeptic_s
```

```{r}
##for the test dataset
#fixed effect
test_skeptic_s$PredictionsPerc0 <- predict(PitchRange_m0, newdata = test_skeptic_s, allow_new_levels = T)[, 1]
test_skeptic_s$Predictions0[test_skeptic_s$PredictionsPerc0 > 0.5] <- "Schizophrenia"
test_skeptic_s$Predictions0[test_skeptic_s$PredictionsPerc0 <= 0.5] <- "Control"

#vary intercept
test_skeptic_s$PredictionsPerc1 <- predict(PitchRange_m1, newdata = test_skeptic_s, allow_new_levels = T)[, 1]
test_skeptic_s$Predictions1[test_skeptic_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
test_skeptic_s$Predictions1[test_skeptic_s$PredictionsPerc1 <= 0.5] <- "Control"

#vary slopes
test_skeptic_s$PredictionsPerc2 <- predict(PitchRange_m2, newdata = test_skeptic_s, allow_new_levels = T)[, 1]
test_skeptic_s$Predictions2[test_skeptic_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
test_skeptic_s$Predictions2[test_skeptic_s$PredictionsPerc2 <= 0.5] <- "Control"


test_skeptic_s <- test_skeptic_s %>% 
  mutate(
    Group = as.factor(Group), 
    Predictions0 = as.factor(Predictions0),
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2)
  )

test_skeptic_s

```



```{r}
## assesing average performance

# for informed
# train
#fixed
conf_mat(
  train_informed_s, 
  truth = Group,
  estimate = Predictions0,
  dnn = c("Prediction", "Truth")
)

metrics(train_informed_s, 
        truth = Group,
        estimate = Predictions0) %>% 
  knitr::kable()

#intercept
conf_mat(
  train_informed_s, 
  truth = Group,
  estimate = Predictions1,
  dnn = c("Prediction", "Truth")
)

metrics(train_informed_s, 
        truth = Group,
        estimate = Predictions1) %>% 
  knitr::kable()


#slope
conf_mat(
  train_informed_s, 
  truth = Group,
  estimate = Predictions2,
  dnn = c("Prediction", "Truth")
)

metrics(train_informed_s, 
        truth = Group,
        estimate = Predictions2) %>% 
  knitr::kable()


# test
#fixed
conf_mat(
  test_informed_s, 
  truth = Group,
  estimate = Predictions0,
  dnn = c("Prediction", "Truth")
)

metrics(test_informed_s, 
        truth = Group,
        estimate = Predictions0) %>% 
  knitr::kable()

#intercept
conf_mat(
  test_informed_s, 
  truth = Group,
  estimate = Predictions1,
  dnn = c("Prediction", "Truth")
)

metrics(test_informed_s, 
        truth = Group,
        estimate = Predictions1) %>% 
  knitr::kable()


#slope
conf_mat(
  test_informed_s, 
  truth = Group,
  estimate = Predictions2,
  dnn = c("Prediction", "Truth")
)

metrics(test_informed_s, 
        truth = Group,
        estimate = Predictions2) %>% 
  knitr::kable()

```

```{r}
##skeptic
# train
#fixed
conf_mat(
  train_skeptic_s, 
  truth = Group,
  estimate = Predictions0,
  dnn = c("Prediction", "Truth")
)

metrics(train_skeptic_s, 
        truth = Group,
        estimate = Predictions0) %>% 
  knitr::kable()

#intercept
conf_mat(
  train_skeptic_s, 
  truth = Group,
  estimate = Predictions1,
  dnn = c("Prediction", "Truth")
)

metrics(train_skeptic_s, 
        truth = Group,
        estimate = Predictions1) %>% 
  knitr::kable()


#slope
conf_mat(
  train_skeptic_s, 
  truth = Group,
  estimate = Predictions2,
  dnn = c("Prediction", "Truth")
)

metrics(train_skeptic_s, 
        truth = Group,
        estimate = Predictions2) %>% 
  knitr::kable()


# test
#fixed
conf_mat(
  test_skeptic_s, 
  truth = Group,
  estimate = Predictions0,
  dnn = c("Prediction", "Truth")
)

metrics(test_skeptic_s, 
        truth = Group,
        estimate = Predictions0) %>% 
  knitr::kable()

#intercept
conf_mat(
  test_skeptic_s, 
  truth = Group,
  estimate = Predictions1,
  dnn = c("Prediction", "Truth")
)

metrics(test_skeptic_s, 
        truth = Group,
        estimate = Predictions1) %>% 
  knitr::kable()


#slope
conf_mat(
  test_skeptic_s, 
  truth = Group,
  estimate = Predictions2,
  dnn = c("Prediction", "Truth")
)

met_slope_test_skeptic <- metrics(test_skeptic_s, 
        truth = Group,
        estimate = Predictions2) %>% 
  knitr::kable()

met_slope_test_skeptic


```

```{r}
#create datafram with the accuracy estimates 
## !! add kap? Uncertainty?? !!

Accuracy <- c(0.965625, 0.968125, 0.9975, 0.97, 0.97, 0.9725, 0.6325, 0.64, 0.9975, 0.57, 0.5725, 0.58)
Data <- c("informed", "informed", "informed", "informed", "informed", "informed", "skeptic", "skeptic", "skeptic", "skeptic", "skeptic", "skeptic")
Type <- c("train", "train", "train", "test", "test", "test", "train", "train", "train", "test", "test", "test")
Model <- c("FixedEffect", "VaryingIntercept", "VaryingSlope", "FixedEffect", "VaryingIntercept", "VaryingSlope", "FixedEffect", "VaryingIntercept", "VaryingSlope", "FixedEffect", "VaryingIntercept", "VaryingSlope")


df <- data.frame(Accuracy, Data, Type, Model)


#plot the data
pacman::p_load("ggrepel")
ggplot(df, aes(x=Model, y=Accuracy, color=Type)) + 
  facet_wrap(~Data, nrow=1) +
   geom_point() +
  geom_label_repel(aes(label = Accuracy,
                    fill = factor(Type)), color = 'white',
                    size = 3.5) +
   theme(legend.position = "bottom") +
  ggtitle('The average performance of the models')



```



```{r}
#Uncertainty
PerformanceProb <-  tibble(expand_grid(
  Sample = seq(2000), 
  Model = c("FixedEffect", "VaryingIntercept", "VaryingSlope"),
  Setup = c("informed", "skeptic"),
  Type = c("train", "test")))
  
train0 <- inv_logit_scaled(posterior_linpred(PitchRange_m0, summary = F)) 
test0 <- inv_logit_scaled(posterior_linpred(PitchRange_m0, summary = F, 
                                            newdata = test_informed_s, allow_new_levels = T))

train1 <- inv_logit_scaled(posterior_linpred(PitchRange_m1, summary = F)) 
test1 <- inv_logit_scaled(posterior_linpred(PitchRange_m1, summary = F, 
                                            newdata = test_informed_s, allow_new_levels = T))

train2 <- inv_logit_scaled(posterior_linpred(PitchRange_m2, summary = F)) 
test2 <- inv_logit_scaled(posterior_linpred(PitchRange_m2, summary = F, 
                                            newdata = test_informed_s, allow_new_levels = T))


train0s <- inv_logit_scaled(posterior_linpred(PitchRange_m0_skeptic, summary = F)) 
test0s <- inv_logit_scaled(posterior_linpred(PitchRange_m0_skeptic, summary = F, 
                                            newdata = test_skeptic_s, allow_new_levels = T))

train1s <- inv_logit_scaled(posterior_linpred(PitchRange_m1_skeptic, summary = F)) 
test1s <- inv_logit_scaled(posterior_linpred(PitchRange_m1_skeptic, summary = F, 
                                            newdata = test_skeptic_s, allow_new_levels = T))

train2s <- inv_logit_scaled(posterior_linpred(PitchRange_m2_skeptic, summary = F)) 
test2s <- inv_logit_scaled(posterior_linpred(PitchRange_m2_skeptic, summary = F, 
                                            newdata = test_skeptic_s, allow_new_levels = T))

PerformanceProb

for (i in seq(2000)) {
  train_informed_s$PredictionsPerc0 <- as.factor(ifelse(train0[i,] > 0.5, "Schizophrenia", "Control"))
  test_informed_s$PredictionsPerc0 <- as.factor(ifelse(test0[i,] > 0.5, "Schizophrenia", "Control"))
  
   train_informed_s$PredictionsPerc1 <- as.factor(ifelse(train1[i,] > 0.5, "Schizophrenia", "Control"))
  test_informed_s$PredictionsPerc1 <- as.factor(ifelse(test1[i,] > 0.5, "Schizophrenia", "Control"))
  
   train_informed_s$PredictionsPerc2 <- as.factor(ifelse(train2[i,] > 0.5, "Schizophrenia", "Control"))
  test_informed_s$PredictionsPerc2 <- as.factor(ifelse(test2[i,] > 0.5, "Schizophrenia", "Control"))
  
    train_skeptic_s$PredictionsPerc0s <- as.factor(ifelse(train0s[i,] > 0.5, "Schizophrenia", "Control"))
  test_skeptic_s$PredictionsPerc0s <- as.factor(ifelse(test0s[i,] > 0.5, "Schizophrenia", "Control"))
  
   train_skeptic_s$PredictionsPerc1s <- as.factor(ifelse(train1s[i,] > 0.5, "Schizophrenia", "Control"))
  test_skeptic_s$PredictionsPerc1s <- as.factor(ifelse(test1s[i,] > 0.5, "Schizophrenia", "Control"))
  
   train_skeptic_s$PredictionsPerc2s <- as.factor(ifelse(train2s[i,] > 0.5, "Schizophrenia", "Control"))
  test_skeptic_s$PredictionsPerc2s <- as.factor(ifelse(test2s[i,] > 0.5, "Schizophrenia", "Control"))
  
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "FixedEffect" & PerformanceProb$Setup == "informed" & PerformanceProb$Type == "train"] <- 
    accuracy(train_informed_s, truth = Group, estimate = PredictionsPerc0)[, ".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "FixedEffect" & PerformanceProb$Setup == "informed" & PerformanceProb$Type == "test"] <- accuracy(test_informed_s, truth = Group, estimate = PredictionsPerc0)[, ".estimate"]
  
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" & PerformanceProb$Setup == "informed" & PerformanceProb$Type == "train"]<-  accuracy(train_informed_s, truth = Group, estimate = PredictionsPerc1)[, ".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" & PerformanceProb$Setup == "informed" & PerformanceProb$Type == "test"] <- accuracy(test_informed_s, truth = Group, estimate = PredictionsPerc1)[, ".estimate"]
  

  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingSlope" & PerformanceProb$Setup == "informed" & PerformanceProb$Type == "train"] <- 
    accuracy(train_informed_s, truth = Group, estimate = PredictionsPerc2)[, ".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingSlope" & PerformanceProb$Setup == "informed" & PerformanceProb$Type == "test"] <- 
    accuracy(test_informed_s, truth = Group, estimate = PredictionsPerc2)[, ".estimate"]
  
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "FixedEffect" & PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "train"] <- 
    accuracy(train_skeptic_s, truth = Group, estimate = PredictionsPerc0s)[, ".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "FixedEffect" & PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "test"] <- 
    accuracy(test_skeptic_s, truth = Group, estimate = PredictionsPerc0s)[, ".estimate"]
  
  
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" & PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "train"] <-
    accuracy(train_skeptic_s, truth = Group, estimate = PredictionsPerc1s)[, ".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" & PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "test"] <- 
    accuracy(test_skeptic_s, truth = Group, estimate = PredictionsPerc1s)[, ".estimate"]
  

  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingSlope" & PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "train"] <- accuracy(train_skeptic_s, truth = Group, estimate = PredictionsPerc2s)[, ".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingSlope" & PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "test"] <- accuracy(test_skeptic_s, truth = Group, estimate = PredictionsPerc2s)[, ".estimate"]
  

}


PerformanceProb$Accuracy <- as.numeric(PerformanceProb$Accuracy)

```


```{r}
#plot

dodge <- .5

ggplot(PerformanceProb, aes(x=Model, y=Accuracy, color=Type)) + 
  facet_wrap(~Setup, nrow=1) +
   geom_point(size=2,
             position = position_dodge(width = dodge)) +
  ggtitle('The uncertainty of the models') 


```



```{r}
#check features

pacman::p_load(kernlab, randomForest, xgboost, knitr, dotwhisker)

d_inf <- train_informed_s %>% 
  mutate(Trial = NULL, Preds = NULL, PredictionsPerc2 = NULL, PredictionsPerc1 = NULL, PredictionsPerc0 = NULL, Predictions0 = NULL, Predictions1 = NULL, Predictions2 = NULL, PredictionsPerc2s = NULL )

LogisticRegression_Inf <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm") %>% 
  fit(Group ~ 1 + Pitch_mode + Pitch_vari + Speech_rate + Spoken_time + Pause_number + Pause_length + noise1 + noise2 + noise3 + noise4 + (1 | ID), data = d_inf)


#tidyverse
pacman::p_load(DALEX, DALEXtra)

explainer_lm <- 
  explain_tidymodels(
    LogisticRegression_Inf, 
    data = train_informed_s, 
    y = as.numeric(train_informed_s$Group) -1, 
    label = "logReg", 
    verbose = FALSE
  )

explainer_lm %>% 
  model_parts() %>% 
  plot(show_boxplots = FALSE) + ggtitle("Feature Importance ", "")


```

- The most important parameters to focus on, when predicting Schizophrenia is The lenght of one's pauses, The time Spoken, and the Speech Rate. 



## Part III - Applying the ML pipeline to empirical data

Download the empirical dataset from brightspace and apply your ML pipeline to the new data, adjusting where needed. Warning: in the simulated dataset we only had 10 features, now you have many more! Such is the life of the ML practitioner. Consider the impact a higher number of features will have on your ML inference, and decide whether you need to cut down the number of features before running the pipeline (or alternatively expand the pipeline to add feature selection).

Data: https://www.dropbox.com/s/7ky1axvea33lgye/Ass3_empiricalData1.csv?dl=0


machine learning pipeline: i) create a data budget (e.g. balanced training and test sets); ii) pre-process the data (e.g. scaling the features); iii) fit and assess a classification algorithm on the training data (e.g. Bayesian multilevel logistic regression); iv) assess performance on the test set; v) discuss whether performance is as expected and feature importance is as expected.


```{r}
 ###Not used - SKIP !!
data <- read.csv("Ass3_empiricalData1.csv")

view(data)
head(data)

```


```{r}
## Factor analysis

#load in packages
pacman::p_load(tidyverse,psych,fmsb,GPArotation)

```

```{r}
#clean data

#remove demographic variables
data <- data %>% 
  select(-Language, -Gender, -Trial, -Corpus)


clean_data <- data %>% select(-NewID)

#make SZ = 1 and CT = 0
clean_data$Diagnosis[clean_data$Diagnosis =="CT"] <- "0"
clean_data$Diagnosis[clean_data$Diagnosis =="SCZ"] <- "1"

clean_data$Diagnosis <- as.numeric(clean_data$Diagnosis)

clean_data[is.na(clean_data)] <- 0
clean_data

#clean_data <- as.data.frame(lapply(clean_data, as.numeric))
str(clean_data)
```


```{r}
# correlational matrix
clean_data_cor <- cor(clean_data)
heatmap(clean_data_cor)


```


```{r}

# find numbers of factors
fa.parallel(clean_data_cor,fa="fa")

```
Interpret the screeplot and conclude how many factors the analysis suggest you to use - dififcult to interpret due to the amoint of variables - but maybe around 10


```{r}
# the factor analysis
fa_10 <- fa(clean_data, 9)
fa_small <- print(fa_10$loadings, cutoff = 0.5)

fa_df <- as.tibble(fa_10)

#plot
fa.diagram(fa_small)


```

### Regression analysis using the factors scores as the independent variable:
Let’s combine the dependent variable and the factor scores into a dataset and label them.

```{r}
#combine the dependent variable and the factor scores into a dataset and label them.
regdata <- cbind(data[2], fa_10$scores)
regdata <- cbind(data[3], regdata)
regdata <- cbind(data[1], regdata)

regdata


```

```{r}
```

```{r}
```

### Feature importance ### 

```{r}
data <- read.csv("Ass3_empiricalData1.csv")
head(data)

```


```{r}
#clean data

library(tidyverse)
#remove demographic variables
non_demo_data <- data %>% 
  select(-Language, -Gender, -Trial, -Corpus, -PatID)


#make the df into a numeric dataframe
non_demo_data <- data.matrix(non_demo_data)

pacman::p_load(caret)
#remove redundant features
findLinearCombos(non_demo_data) 


non_demo_data <- as.data.frame(non_demo_data)

# to be removed: 9  18  20  22 320, 399
indices <- c(9, 18, 20, 22, 320, 388)
clean_data <- non_demo_data[-indices]

head(clean_data)

#diagnosis: 1 = CT, 2 = SCZ
clean_data$Diagnosis <- as.factor(clean_data$Diagnosis)


#other highly correlated variables? 
pacman::p_load(corrplot)


cor_data <- clean_data %>% 
  select(-NewID)

cor_data$Diagnosis <- as.numeric(cor_data$Diagnosis)

clean_data.cor <-  cor(cor_data)

 # Modify correlation matrix
cor_matrix_rm <- clean_data.cor                 
cor_matrix_rm[upper.tri(cor_matrix_rm)] <- 0
diag(cor_matrix_rm) <- 0
cor_matrix_rm

# Remove highly correlated variables
data_new <- clean_data[ , !apply(cor_matrix_rm,    
                           2,
                           function(x) any(x > 0.8))]


head(data_new)  
head(data)
#before we had 398 variables 
#we now have 137 variable 

heatmap(clean_data.cor)

view(data_new)


```

```{r}

#get diagnosis back 
data_new <- cbind(data[3], data_new)
head(data_new)

#data budgetting
TestID <- sample(unique(data_new$NewID),length(unique(data_new$NewID))*0.2)
TestID

length(unique(data_new$NewID))

train_data <- data_new %>% 
  subset(!(NewID %in% TestID))

test_data <- data_new %>% 
  subset(NewID %in% TestID)

test_data
train_data

```



```{r}

# 2) preprocessing - scaling the training data 

pacman::p_load(tidymodels)

rec <- train_data %>% 
  recipe(Diagnosis ~ .) %>% 
  step_scale(.) %>% 
  step_center(.) %>% 
  prep(training = train_data, retain = TRUE)

train_data_s <- juice(rec)
test_data_s <- bake(rec, new_data = test_data)

train_data_s
```


```{r}
#check features

pacman::p_load(kernlab, randomForest, xgboost, knitr, dotwhisker)

d_inf <- train_data_s %>% 
  mutate(NewID = NULL)

LogisticRegression_Inf <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm") %>% 
  fit(Diagnosis ~ ., data = d_inf)


#tidyverse
pacman::p_load(DALEX, DALEXtra)

explainer_lm <- 
  explain_tidymodels(
    LogisticRegression_Inf, 
    data = train_data_s, 
    y = as.numeric(train_data_s$Diagnosis) -1, 
    label = "logReg", 
    verbose = FALSE
  )


fi <- model_parts(explainer_lm)

plot <- fi %>%  
  plot(show_boxplots = FALSE) + ggtitle("Feature Importance ", "")

#select top 40 highest variables
top40 <- train_data_s %>% 
  select(Diagnosis, TurnNumber_Cova, Duration_Praat, QOQ_MAD, QOQ_IQR, MCEP18_MAD, MCEP21_MAD, MCEP20_MAD, MCEP21_IQR, HMPDD0_Mean, CreakProbability_Mean, peakSlope_Mean, MCEP7_MAD, F0_Mean_Praat, HMPDM11_IQR, HMPDD0_IQR, HMPDD0_IQR, HRF_IQR,  MCEP20_IQR, HMPDD5_Mean, MCEP17_IQR, HRF_MAD,  VAD_Mean, MCEP17_MAD, MCEP19_IQR, MCEP8_MAD, MCEP3_MAD, MCEP10_IQR, MCEP14_IQR, MCEP10_MAD, MCEP7_IQR, HMPDD4_MAD, HMPDD4_MAD, HMPDM22_MAD, MCEP4_IQR, Rd_IQR, MCEP16_IQR, MCEP4_MAD, MCEP1_Mean, MCEP12_IQR, Harmonicity_Mean, HMPDD12_IQR, MCEP14_MAD, MCEP2_MAD )
   
LogisticRegression_Inf40 <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm") %>% 
  fit(Diagnosis ~ ., data = top40)

explainer_lm40 <- 
  explain_tidymodels(
    LogisticRegression_Inf40, 
    data = top40, 
    y = as.numeric(top40$Diagnosis) -1, 
    label = "logReg", 
    verbose = FALSE
  )


fi40 <- model_parts(explainer_lm40)

 fi40 %>%  
  plot(show_boxplots = FALSE) + ggtitle("Feature Importance top 40 ", "")



```

```{r}
#we can remove the last 10
top30_train <- train_data_s %>% 
  select(Diagnosis, NewID, TurnNumber_Cova, Duration_Praat, QOQ_MAD, QOQ_IQR, MCEP18_MAD, MCEP21_MAD, MCEP21_IQR, HMPDD0_Mean, peakSlope_Mean,F0_Mean_Praat, HMPDM11_IQR, HMPDD0_IQR, HMPDD0_IQR, HRF_IQR, HMPDD5_Mean, MCEP17_IQR, HRF_MAD, MCEP17_MAD, MCEP8_MAD,MCEP10_IQR, MCEP14_IQR, MCEP10_MAD, MCEP7_IQR, HMPDD4_MAD, HMPDD4_MAD, HMPDM22_MAD, MCEP4_IQR, MCEP16_IQR, MCEP4_MAD, MCEP1_Mean, Harmonicity_Mean, HMPDD12_IQR, MCEP14_MAD, MCEP2_MAD )
   
top30_test <- test_data_s %>% 
  select(Diagnosis, NewID, TurnNumber_Cova, Duration_Praat, QOQ_MAD, QOQ_IQR, MCEP18_MAD, MCEP21_MAD, MCEP21_IQR, HMPDD0_Mean, peakSlope_Mean,F0_Mean_Praat, HMPDM11_IQR, HMPDD0_IQR, HMPDD0_IQR, HRF_IQR, HMPDD5_Mean, MCEP17_IQR, HRF_MAD, MCEP17_MAD, MCEP8_MAD,MCEP10_IQR, MCEP14_IQR, MCEP10_MAD, MCEP7_IQR, HMPDD4_MAD, HMPDD4_MAD, HMPDM22_MAD, MCEP4_IQR, MCEP16_IQR, MCEP4_MAD, MCEP1_Mean, Harmonicity_Mean, HMPDD12_IQR, MCEP14_MAD, MCEP2_MAD )



```

Feature importance: It can be used for feature selection by evaluating the Information gain of each variable in the context of the target variable.


```{r}
## ML pipeline!

###define formula
## Create a formula for a model with a large number of variables:

library(brms)
#varying intercept
model <- bf(Diagnosis ~ . + (1|NewID))


#get prior
#view(get_prior(model, train_data_s, family = bernoulli))



 # intercept
model_prior <- c(
   brms::prior(normal(0, 1), class = Intercept), 
   brms::prior(normal(0, 1), class = sd),
  brms:: prior(normal(0, 1), class = b)
  )


# update the file of cmdstanr install
library(cmdstanr) 
cpp_options <- list( "CXXFLAGS+= -fbracket-depth=1024" ) 
cmdstan_make_local(cpp_options = cpp_options)

prior_m <- 
  brm(
    model, 
    data = top30_train,
    family = bernoulli,
    prior = model_prior ,  
    sample_prior = "only", 
    iter = 2000,
    warmup = 500,
    backend = "cmdstanr",
    threads = threading(2),
    cores = 2,
    chains = 2,
    control = list(adapt_delta = 0.9, max_treedepth = 20))

pp_check(prior_m, ndraws = 100) + ggtitle('Intercept model, Prior-predictive check') +xlim(-0.25, 1.25)
```

```{r}

```

```{r}
#fit the model
posterior_m <- brm(
  model, 
  top30_train, 
  family = bernoulli, 
  prior = model_prior , 
  sample_prior = T,
  backend = "cmdstanr", 
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta = 0.9, 
                 max_treedepth = 20)
  )


#pp-check
p2 <- pp_check(posterior_m, ndraws = 100)+ ggtitle('Real data - Posterior-predictive check') + xlim(-0.5, 1.8)
p2


```


```{r}

#update plot
posterior_draws <- as_draws_df(posterior_m)
glimpse(posterior_draws)

# Posterior for the informed
p111 <- ggplot(posterior_draws) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
  labs(title = "Posterior intercept") +
  theme_minimal()

p222 <- ggplot(posterior_draws) +
  geom_density(aes(sd_NewID__Intercept), fill="blue", alpha=0.3) +
  geom_density(aes(prior_sd_NewID), fill="red", alpha=0.3) +
  labs(title = "Posterior SD") +
  theme_minimal()


library(gridExtra)
grid.arrange(p111, p222,  nrow=2,
             top="Update plots, Random intercepts model, real data")

```


```{r}
#generate average prediction

#train data
top30_train$PredictionsPerc <- predict(posterior_m)[, 1]
#top30_train$Predictions <- as.character(top30_train$Predictions)
top30_train$Predictions[top30_train$PredictionsPerc > 0.5] <- "SCZ"
top30_train$Predictions[top30_train$PredictionsPerc <= 0.5] <- "CT"

#test data
top30_test$PredictionsPerc <- predict(posterior_m, newdata = top30_test, allow_new_levels = T)[, 1]
#top30_test$Predictions <- as.character(top30_test$Predictions)
top30_test$Predictions[top30_test$PredictionsPerc > 0.5] <- "SCZ"
top30_test$Predictions[top30_test$PredictionsPerc <= 0.5] <- "CT"


top30_train <- top30_train %>% 
  mutate(
    Diagnosis = as.factor(Diagnosis), 
    Predictions = as.factor(Predictions)
  )

top30_test <- top30_test %>% 
  mutate(
    Diagnosis = as.factor(Diagnosis), 
    Predictions = as.factor(Predictions)
  )

top30_test


```

```{r}
##accuracy
#intercept
#train
conf_mat(
  top30_train, 
  truth = Diagnosis,
  estimate = Predictions,
  dnn = c("Prediction", "Truth")
)

metrics(top30_train, 
        truth = Diagnosis,
        estimate = Predictions) %>% 
  knitr::kable()

#test
conf_mat(
  top30_test, 
  truth = Diagnosis,
  estimate = Predictions,
  dnn = c("Prediction", "Truth")
)

metrics(top30_test, 
        truth = Diagnosis,
        estimate = Predictions) %>% 
  knitr::kable()
```


```{r}
Accuracy <- c(1, 0.5989305)
Type <- c("train", "test")
Model <- "VaryingIntercept"


df <- data.frame(Accuracy, Type, Model)


#plot the data
pacman::p_load("ggrepel")
ggplot(df, aes(x=Type, y=Accuracy, color=Model)) + 
   geom_point() +
  geom_label_repel(aes(label = Accuracy,
                    size = 3.5)) +
   theme(legend.position = "bottom") +
  ggtitle('The average performance of the model')

```


```{r}

#Uncertainty
PerformanceProb1 <-  tibble(expand_grid(
  Sample = seq(2000), 
  Model = "VaryingIntercept",
  Type = c("train", "test")))
  
train <- inv_logit_scaled(posterior_linpred(posterior_m, summary = F)) 
test <- inv_logit_scaled(posterior_linpred(posterior_m, summary = F, 
                                            newdata = top30_test, allow_new_levels = T))

PerformanceProb

top30_test

for (i in seq(2000)) {
  top30_train$PredictionsPerc <- as.factor(ifelse(train[i,] > 0.5, "SCZ", "CT"))
  top30_test$PredictionsPerc <- as.factor(ifelse(test[i,] > 0.5, "SCZ", "CT"))
  
  
  PerformanceProb1$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept"  & PerformanceProb$Type == "train"] <- 
    accuracy(top30_train, truth = Diagnosis, estimate = PredictionsPerc)[, ".estimate"]
  
  PerformanceProb1$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" & PerformanceProb$Type == "test"] <- accuracy(top30_test, truth = Diagnosis, estimate = PredictionsPerc)[, ".estimate"]
  
}


PerformanceProb1$Accuracy <- as.numeric(PerformanceProb1$Accuracy)

```
```{r}
#plot

dodge <- .5

ggplot(PerformanceProb1, aes(x=Model, y=Accuracy, color=Type)) + 
   geom_point(size=2,
             position = position_dodge(width = dodge)) +
  ggtitle('The uncertainty of the model') 

```

