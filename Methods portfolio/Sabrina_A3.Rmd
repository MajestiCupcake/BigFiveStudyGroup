
---
title: "A3_Machine-learning.Rmd"
output: html_document
date: "2022-11-02"
---

# Assignment 3 - Machine learning

The Machine Learning assignment has 3 main parts: First we create a skeptical and an informed simulation, based on the meta-analysis. Second we build and test our machine learning pipeline on the simulated data. Second we apply the pipeline to the empirical data.

The report for the exam, thus, consists of the answer to all the following prompts:
- Describe your machine learning pipeline. Produce a diagram of it to guide the reader (e.g. see Rybner et al 2022 Vocal markers of autism: Assessing the generalizability of ML models), and describe the different parts: data budgeting, data preprocessing, model choice and training, assessment of performance.
- Briefly justify and describe your use of simulated data, and results from the pipeline on them.
- Describe results from applying the ML pipeline to the empirical data and what can we learn from them.

Remember: plots are very very important to communicate your process and results.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
 
### Loading packages
```{r}
pacman::p_load(tidyverse,tidybayes,brms,ggplot2,bayesplot,rstan,gridExtra,grid,dplyr,cmdstanr,msm,metafor,janitor,ggridges,glue,stringr,forcats,tidymodels)

# install.packages("remotes")
#remotes::install_github("stan-dev/cmdstanr")

#cmdstanr::install_cmdstan()
```

## Part I - Simulating data

Use the meta-analysis reported in Parola et al (2020),
- create a simulated dataset with 100 matched pairs of schizophrenia and controls
- each participant producing 10 repeated measures (10 trials with their speech recorded).
- for each of these "recordings" (data points) produce 10 acoustic measures:
- 6 from the meta-analysis,
- 4 with just random noise.

Do the same for a baseline dataset including only 10 noise variables. Tip: see the slides for the code. 

### Simulating the data
```{r}
# First define population size
set.seed(169)
n <- 100
trials <- 10


# Define the different effect sizes
InformedEffectMean <- c(0.25,-0.55,-0.75,-1.26,0.05,1.89,0,0,0,0)
SkepticalEffectMean <- rep(0,10)


# Then we define individual variability from population and across trails and measurement error
IndividualSD <- 1
TrialSD <- 0.5
Error <- 0.2


# For each pair of participants we need to identify the true effect size for each variable

for (i in seq(10)){
  temp_informed <- tibble(
    ID = seq(n),
    TrueEffect = rnorm(n,InformedEffectMean[i],IndividualSD),
    Variable = paste0("v",i))
  temp_skeptic <- tibble(
    ID = seq(n),
    TrueEffect = rnorm(n,SkepticalEffectMean[i],IndividualSD),
    Variable = paste0("v",i))
  if (i == 1) {
    d_informed_true <- temp_informed
    d_skeptic_true <- temp_skeptic

  } else {
    d_informed_true <- rbind(d_informed_true,temp_informed)
    d_skeptic_true <- rbind(d_skeptic_true,temp_skeptic)
    }
}

```


```{r}
# Create tibble with one row per trial
d_trial <- tibble(expand_grid(ID = seq(n),Trial = seq(trials),Group = c("Schizophrenia","Control")))

d_informed <- merge(d_informed_true,d_trial)
d_skeptic <- merge(d_skeptic_true,d_trial)

d_informed$Variable[d_informed$Variable=="v1"] <- "Pitch_mode"
d_informed$Variable[d_informed$Variable=="v2"] <- "Pitch_vari"
d_informed$Variable[d_informed$Variable=="v3"] <- "Speech_rate"
d_informed$Variable[d_informed$Variable=="v4"] <- "Prop_spokenT"
d_informed$Variable[d_informed$Variable=="v5"] <- "Pause_n"
d_informed$Variable[d_informed$Variable=="v6"] <- "Pause_len"
d_informed$Variable[d_informed$Variable=="v7"] <- "Noise1"
d_informed$Variable[d_informed$Variable=="v8"] <- "Noise2"
d_informed$Variable[d_informed$Variable=="v9"] <- "Noise3"
d_informed$Variable[d_informed$Variable=="v10"] <- "Noise4"

d_skeptic$Variable[d_skeptic$Variable=="v1"] <- "Pitch_mode"
d_skeptic$Variable[d_skeptic$Variable=="v2"] <- "Pitch_vari"
d_skeptic$Variable[d_skeptic$Variable=="v3"] <- "Speech_rate"
d_skeptic$Variable[d_skeptic$Variable=="v4"] <- "Prop_spokenT"
d_skeptic$Variable[d_skeptic$Variable=="v5"] <- "Pause_n"
d_skeptic$Variable[d_skeptic$Variable=="v6"] <- "Pause_len"
d_skeptic$Variable[d_skeptic$Variable=="v7"] <- "Noise1"
d_skeptic$Variable[d_skeptic$Variable=="v8"] <- "Noise2"
d_skeptic$Variable[d_skeptic$Variable=="v9"] <- "Noise3"
d_skeptic$Variable[d_skeptic$Variable=="v10"] <- "Noise4"

for (i in seq(nrow(d_informed))){
  d_informed$measurement[i] <- ifelse(d_informed$Group[i]=="Schizophrenia",
                                      rnorm(1,rnorm(1,d_informed$TrueEffect[i]/2,TrialSD),Error),
                                      rnorm(1,rnorm(1,(-d_informed$TrueEffect[i])/2,TrialSD),Error))
  d_skeptic$measurement[i] <- ifelse(d_skeptic$Group[i]=="Schizophrenia",
                                      rnorm(1,rnorm(1,d_skeptic$TrueEffect[i]/2,TrialSD),Error),
                                      rnorm(1,rnorm(1,(-d_skeptic$TrueEffect[i])/2,TrialSD),Error))
}


d_informed_wide <- d_informed %>% 
  mutate(TrueEffect = NULL) %>% 
  pivot_wider(names_from = Variable,
              values_from = measurement)

d_skeptic_wide <- d_skeptic %>% 
  mutate(TrueEffect = NULL) %>% 
  pivot_wider(names_from = Variable,
              values_from = measurement)
```
### Plotting simulated data
```{r}
densplotinformed <- ggplot(d_informed) +
  aes(x = measurement, fill = Group,alpha=.5) +
  geom_density(adjust = 1L) +
  scale_fill_hue(direction = 1) +
  theme_gray() +
  facet_wrap(vars(Variable), ncol = 2L) +
  guides(alpha = FALSE) +
  labs(title = "Informed effect mean") 

densplotskeptic <- ggplot(d_skeptic) +
  aes(x = measurement, fill = Group,alpha=.5) +
  geom_density(adjust = 1L) +
  scale_fill_hue(direction = 1) +
  theme_gray() +
  facet_wrap(vars(Variable), ncol = 2L) +
  guides(alpha = FALSE) +
  labs(title = "Skeptical effect mean") 

densplotinformed
densplotskeptic
```


## Part II - ML pipeline on simulated data

On the two simulated datasets (separately) build a machine learning pipeline:

i) create a data budget (e.g. balanced training and test sets);

ii) pre-process the data (e.g. scaling the features);

iii) fit and assess a classification algorithm on the training data (e.g. Bayesian multilevel logistic regression);

iv) assess performance on the test set;

v) discuss whether performance is as expected and feature importance is as expected.

Bonus question: replace the bayesian multilevel regression with a different algorithm, e.g. SVM or random forest (but really, anything you'd like to try).

### Creating data budget
```{r}
# Creating a data budget for informed
TestID <- sample(seq(n),20)

train_informed <- d_informed_wide %>% 
  subset(!(ID %in% TestID))

test_informed <- d_informed_wide %>% 
  subset(ID %in% TestID)

# Creating a data budget for skeptical
train_skeptic <- d_skeptic_wide %>% 
  subset(!(ID %in% TestID))

test_skeptic <- d_skeptic_wide %>% 
  subset(ID %in% TestID)
```

### Pre-proccesing the data
```{r}
# Pre-processing the data
library(tidymodels)

rec_informed <- train_informed %>% 
  recipe(Group~.) %>% # defines the outcome
  step_scale("Pitch_mode","Pitch_vari","Speech_rate","Prop_spokenT","Pause_n","Pause_len","Noise1","Noise2","Noise3","Noise4") %>% # scales numeric predictors
  step_center("Pitch_mode","Pitch_vari","Speech_rate","Prop_spokenT","Pause_n","Pause_len","Noise1","Noise2","Noise3","Noise4") %>% # center numeric predictors
  prep(training = train_informed,retain=TRUE)

rec_skeptic <- train_skeptic %>% 
  recipe(Group ~ . ) %>% 
  step_scale("Pitch_mode","Pitch_vari","Speech_rate","Prop_spokenT","Pause_n","Pause_len","Noise1","Noise2","Noise3","Noise4") %>% 
  step_center("Pitch_mode","Pitch_vari","Speech_rate","Prop_spokenT","Pause_n","Pause_len","Noise1","Noise2","Noise3","Noise4") %>% 
  prep(training = train_skeptic,retain=TRUE)

# Apply recipe to train and test for informed
train_informed_s <- juice(rec_informed)
test_informed_s <- bake(rec_informed,new_data=test_informed)

# Apply recipe to train and test for skeptic
train_skeptic_s <- juice(rec_skeptic)
test_skeptic_s <- bake(rec_skeptic,new_data=test_skeptic)
```

### Fitting a classifcation algorithm on the traning data
```{r}
# fitting and assess a classification algorithm on the training data (e.g. Bayesian multilevel logistic regression)

# Formulas used for both models
Variables_f1 <- bf(Group~1+Pitch_mode+Pitch_vari+Speech_rate+Prop_spokenT+Pause_n+Pause_len+ Noise1+Noise2+Noise3+Noise4)

Variables_f2 <- bf(Group~1+Pitch_mode+Pitch_vari+Speech_rate+Prop_spokenT+Pause_n+Pause_len+ Noise1+Noise2+Noise3+Noise4+(1|ID))

Variables_f3 <- bf(Group~1+Pitch_mode+Pitch_vari+Speech_rate+Prop_spokenT+Pause_n+Pause_len+ Noise1+Noise2+Noise3+Noise4+(Pitch_mode+Pitch_vari+Speech_rate+Prop_spokenT+Pause_n+Pause_len+Noise1+Noise2+Noise3+Noise4|ID))

# Get prior for informed
get_prior(Variables_f3,train_informed_s,family=bernoulli)

# Priors
Variables_p1 <- c(
  prior(normal(0,1),class=Intercept),
  prior(normal(0,1),class=b)
)

Variables_p2 <- c(
  prior(normal(0,1),class=Intercept),
  prior(normal(0,1),class=b),
  prior(normal(0,.1),class=sd)
)

Variables_p3 <- c(
  prior(normal(0,1),class=Intercept),
  prior(normal(0,1),class=b),
  prior(normal(0,.1),class=sd),
  prior(lkj(1),class = cor),
  prior(lkj(1),class = cor,group = ID)
)

Variables_m1i <- brm(
  Variables_f1,
  train_informed_s,
  family = bernoulli,
  prior = Variables_p1,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta=0.9,
                 max_treedepth = 20)
)

Variables_m2i <- brm(
  Variables_f2,
  train_informed_s,
  family = bernoulli,
  prior = Variables_p2,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta=0.9,
                 max_treedepth = 20)
)

Variables_m3i <- brm(
  Variables_f3,
  train_informed_s,
  family = bernoulli,
  prior = Variables_p3,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta=0.9,
                 max_treedepth = 20)
)

Variables_m1s <- brm(
  Variables_f1,
  train_skeptic_s,
  family = bernoulli,
  prior = Variables_p1,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta=0.9,
                 max_treedepth = 20)
)

Variables_m2s <- brm(
  Variables_f2,
  train_skeptic_s,
  family = bernoulli,
  prior = Variables_p2,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta=0.9,
                 max_treedepth = 20)
)

Variables_m3s <- brm(
  Variables_f3,
  train_skeptic_s,
  family = bernoulli,
  prior = Variables_p3,
  sample_prior = T,
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  threads = threading(2),
  control = list(adapt_delta=0.9,
                 max_treedepth = 20)
)


pp_check(Variables_m1i, ndraws = 100) + labs(title="Prior predictive checks informed, Fixed effects")
pp_check(Variables_m2i, ndraws = 100) + labs(title="Prior predictive checks informed, Varrying intercepts")
pp_check(Variables_m3i, ndraws = 100) + labs(title="Prior predictive checks informed, Varrying slopes")

pp_check(Variables_m1s, ndraws = 100) + labs(title="Prior predictive checks skeptic, Fixed effects")
pp_check(Variables_m2s, ndraws = 100) + labs(title="Prior predictive checks skeptic, Varrying intercepts")
pp_check(Variables_m3s, ndraws = 100) + labs(title="Prior predictive checks skeptic, Varrying slopes")
```

### Doing the fit, posterior
```{r}
Variables_fit1i<-
  brm(
    Variables_f1,
    train_informed_s,
    save_pars = save_pars(all = TRUE),
    family = bernoulli,
    prior = Variables_p1,
    file = "Variables_fit1i",
    #refit = "on_change",
    sample_prior = T,
    iter = 5000, 
    warmup = 2000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(4),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15 ),
    stan_model_args=list(stanc_options = list("O1"))
  )

Variables_fit2i<-
  brm(
    Variables_f2,
    train_informed_s,
    save_pars = save_pars(all = TRUE),
    family = bernoulli,
    prior = Variables_p2,
    file = "Variables_fit2i",
    #refit = "on_change",
    sample_prior = T,
    iter = 5000, 
    warmup = 2000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(4),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15 ),
    stan_model_args=list(stanc_options = list("O1"))
  )

Variables_fit3i<-
  brm(
    Variables_f3,
    train_informed_s,
    save_pars = save_pars(all = TRUE),
    family = bernoulli,
    prior = Variables_p3,
    file = "Variables_fit3i",
    #refit = "on_change",
    sample_prior = T,
    iter = 5000, 
    warmup = 2000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(4),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15 ),
    stan_model_args=list(stanc_options = list("O1"))
  )

Variables_fit1s<-
  brm(
    Variables_f1,
    train_skeptic_s,
    save_pars = save_pars(all = TRUE),
    family = bernoulli,
    prior = Variables_p1,
    file = "Variables_fit1s",
    #refit = "on_change",
    sample_prior = T,
    iter = 5000, 
    warmup = 2000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(4),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15 ),
    stan_model_args=list(stanc_options = list("O1"))
  )

Variables_fit2s<-
  brm(
    Variables_f2,
    train_skeptic_s,
    save_pars = save_pars(all = TRUE),
    family = bernoulli,
    prior = Variables_p2,
    file = "Variables_fit2s",
    #refit = "on_change",
    sample_prior = T,
    iter = 5000, 
    warmup = 2000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(4),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15 ),
    stan_model_args=list(stanc_options = list("O1"))
  )

Variables_fit3s<-
  brm(
    Variables_f3,
    train_informed_s,
    save_pars = save_pars(all = TRUE),
    family = bernoulli,
    prior = Variables_p3,
    file = "Variables_fit3s",
    #refit = "on_change",
    sample_prior = T,
    iter = 5000, 
    warmup = 2000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(4),
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 15 ),
    stan_model_args=list(stanc_options = list("O1"))
  )
```

### Plots drawing from posterior, posterior update plots
```{r}
posterior1i <- as_draws_df(Variables_fit1i)
posterior2i <- as_draws_df(Variables_fit2i)
posterior3i <- as_draws_df(Variables_fit3i)

posterior1s <- as_draws_df(Variables_fit1s)
posterior2s <- as_draws_df(Variables_fit2s)
posterior3s <- as_draws_df(Variables_fit3s)

# Posterior for the informed
ggplot(posterior1i) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
  labs(title = "Posterior intercept, informed, model 1")
  theme_minimal()

ggplot(posterior1i) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(b_Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(b_Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(b_Prop_spokenT), fill="purple", alpha=0.3) +
  geom_density(aes(b_Pause_n), fill="pink", alpha=0.3) +
  geom_density(aes(b_Noise1), fill="green", alpha=0.3) +
  geom_density(aes(b_Noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(b_Noise3), fill="black", alpha=0.3) +
  geom_density(aes(b_Noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior b, informed, model 1") +
  theme_minimal()

ggplot(posterior2i) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
  labs(title = "Posterior intercept, informed, model 2")
  theme_minimal()

ggplot(posterior2i) +
  geom_density(aes(sd_ID__Intercept), fill="blue", alpha=0.3) +
  geom_density(aes(prior_sd_ID), fill="red", alpha=0.3) +
  labs(title = "Posterior SD, informed, model 2") +
  theme_minimal()

ggplot(posterior2i) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(b_Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(b_Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(b_Prop_spokenT), fill="purple", alpha=0.3) +
  geom_density(aes(b_Pause_n), fill="pink", alpha=0.3) +
  geom_density(aes(b_Noise1), fill="green", alpha=0.3) +
  geom_density(aes(b_Noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(b_Noise3), fill="black", alpha=0.3) +
  geom_density(aes(b_Noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior b, informed, model 2") +
  theme_minimal()

ggplot(posterior3i) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
  labs(title = "Posterior intercept, informed, model 3")
  theme_minimal()

ggplot(posterior3i) +
  geom_density(aes(sd_ID__Intercept), fill="blue", alpha=0.3) +
  geom_density(aes(prior_sd_ID), fill="red", alpha=0.3) +
  labs(title = "Posterior SD, informed, model 3") +
  theme_minimal()

ggplot(posterior3i) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(b_Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(b_Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(b_Prop_spokenT), fill="purple", alpha=0.3) +
  geom_density(aes(b_Pause_n), fill="pink", alpha=0.3) +
  geom_density(aes(b_Noise1), fill="green", alpha=0.3) +
  geom_density(aes(b_Noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(b_Noise3), fill="black", alpha=0.3) +
  geom_density(aes(b_Noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior b, informed, model 3") +
  theme_minimal()

ggplot(posterior3i) +
  geom_density(aes(prior_sd_ID), fill="red") +
  geom_density(aes(sd_ID__Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(sd_ID__Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(sd_ID__Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(sd_ID__Prop_spokenT), fill="purple", alpha=0.3) +
  geom_density(aes(sd_ID__Pause_n), fill="pink", alpha=0.3) +
  geom_density(aes(sd_ID__Noise1), fill="green", alpha=0.3) +
  geom_density(aes(sd_ID__Noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(sd_ID__Noise3), fill="black", alpha=0.3) +
  geom_density(aes(sd_ID__Noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior sd for ID, informed, model 3") +
  theme_minimal()

# Posterior for the skeptic
ggplot(posterior1s) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
  labs(title = "Posterior intercept, skeptic, model 1")
  theme_minimal()

ggplot(posterior1s) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(b_Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(b_Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(b_Prop_spokenT), fill="purple", alpha=0.3) +
  geom_density(aes(b_Pause_n), fill="pink", alpha=0.3) +
  geom_density(aes(b_Noise1), fill="green", alpha=0.3) +
  geom_density(aes(b_Noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(b_Noise3), fill="black", alpha=0.3) +
  geom_density(aes(b_Noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior b, skeptic, model 1") +
  theme_minimal()

ggplot(posterior2s) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
  labs(title = "Posterior intercept, skeptic, model 2")
  theme_minimal()

ggplot(posterior2s) +
  geom_density(aes(sd_ID__Intercept), fill="blue", alpha=0.3) +
  geom_density(aes(prior_sd_ID), fill="red", alpha=0.3) +
  labs(title = "Posterior SD, skeptic, model 2") +
  theme_minimal()

ggplot(posterior2s) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(b_Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(b_Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(b_Prop_spokenT), fill="purple", alpha=0.3) +
  geom_density(aes(b_Pause_n), fill="pink", alpha=0.3) +
  geom_density(aes(b_Noise1), fill="green", alpha=0.3) +
  geom_density(aes(b_Noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(b_Noise3), fill="black", alpha=0.3) +
  geom_density(aes(b_Noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior b, skeptic, model 2") +
  theme_minimal()

ggplot(posterior3s) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
  labs(title = "Posterior intercept, skeptic, model 3")
  theme_minimal()

ggplot(posterior3s) +
  geom_density(aes(sd_ID__Intercept), fill="blue", alpha=0.3) +
  geom_density(aes(prior_sd_ID), fill="red", alpha=0.3) +
  labs(title = "Posterior SD, skeptic, model 3") +
  theme_minimal()

ggplot(posterior3s) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(b_Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(b_Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(b_Prop_spokenT), fill="purple", alpha=0.3) +
  geom_density(aes(b_Pause_n), fill="pink", alpha=0.3) +
  geom_density(aes(b_Noise1), fill="green", alpha=0.3) +
  geom_density(aes(b_Noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(b_Noise3), fill="black", alpha=0.3) +
  geom_density(aes(b_Noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior b, informed,  model 3") +
  theme_minimal()

ggplot(posterior3s) +
  geom_density(aes(prior_sd_ID), fill="red") +
  geom_density(aes(sd_ID__Pitch_mode), fill="blue", alpha=0.3) +
  geom_density(aes(sd_ID__Pitch_vari), fill="salmon", alpha=0.3) +
  geom_density(aes(sd_ID__Speech_rate), fill="orange", alpha=0.3) +
  geom_density(aes(sd_ID__Prop_spokenT), fill="purple", alpha=0.3) +
  geom_density(aes(sd_ID__Pause_n), fill="pink", alpha=0.3) +
  geom_density(aes(sd_ID__Noise1), fill="green", alpha=0.3) +
  geom_density(aes(sd_ID__Noise2), fill="yellow", alpha=0.3) +
  geom_density(aes(sd_ID__Noise3), fill="black", alpha=0.3) +
  geom_density(aes(sd_ID__Noise4), fill="grey", alpha=0.3) +
  labs(title = "Posterior sd for ID, skeptic, model 3") +
  theme_minimal()
```


### Sensitivy analysis if had time
```{r}

```

### Generating average predictions
```{r}
# generate average predictions 
# informed for the trained dataset
# fixed effect
train_informed_s$PredictionsPerc1 <- predict(Variables_fit1i)[, 1]
train_informed_s$Predictions1[train_informed_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
train_informed_s$Predictions1[train_informed_s$PredictionsPerc1 <= 0.5] <- "Control"

#vary intercept
train_informed_s$PredictionsPerc2 <- predict(Variables_fit2i)[, 1]
train_informed_s$Predictions2[train_informed_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
train_informed_s$Predictions2[train_informed_s$PredictionsPerc2 <= 0.5] <- "Control"

#vary slopes
train_informed_s$PredictionsPerc3 <- predict(Variables_fit3i)[, 1]
train_informed_s$Predictions3[train_informed_s$PredictionsPerc3 > 0.5] <- "Schizophrenia"
train_informed_s$Predictions3[train_informed_s$PredictionsPerc3 <= 0.5] <- "Control"


train_informed_s <- train_informed_s %>% 
  mutate(
    Group = as.factor(Group), 
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2),
    Predictions3 = as.factor(Predictions3)
  )

train_informed_s

# generate average predictions 
### skeptic
##for the trained dataset

# fixed effect
train_skeptic_s$PredictionsPerc1 <- predict(Variables_fit1s)[, 1]
train_skeptic_s$Predictions1[train_skeptic_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
train_skeptic_s$Predictions1[train_skeptic_s$PredictionsPerc1 <= 0.5] <- "Control"

#vary intercept
train_skeptic_s$PredictionsPerc2 <- predict(Variables_fit2i)[, 1]
train_skeptic_s$Predictions2[train_skeptic_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
train_skeptic_s$Predictions2[train_skeptic_s$PredictionsPerc2 <= 0.5] <- "Control"

#vary slopes
train_skeptic_s$PredictionsPerc3 <- predict(Variables_fit3i)[, 1]
train_skeptic_s$Predictions3[train_skeptic_s$PredictionsPerc3 > 0.5] <- "Schizophrenia"
train_skeptic_s$Predictions3[train_skeptic_s$PredictionsPerc3 <= 0.5] <- "Control"


train_skeptic_s <- train_skeptic_s %>% 
  mutate(
    Group = as.factor(Group), 
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2),
    Predictions3 = as.factor(Predictions3)
  )

train_skeptic_s
```
### Generate average predictions for informed
```{r}
# generate average predictions 
# informed for the trained dataset
# fixed effect
train_informed_s$PredictionsPerc1 <- predict(Variables_fit1i)[, 1]
train_informed_s$Predictions1[train_informed_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
train_informed_s$Predictions1[train_informed_s$PredictionsPerc1 <= 0.5] <- "Control"

#vary intercept
train_informed_s$PredictionsPerc2 <- predict(Variables_fit2i)[, 1]
train_informed_s$Predictions2[train_informed_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
train_informed_s$Predictions2[train_informed_s$PredictionsPerc2 <= 0.5] <- "Control"

#vary slopes
train_informed_s$PredictionsPerc3 <- predict(Variables_fit3i)[, 1]
train_informed_s$Predictions3[train_informed_s$PredictionsPerc3 > 0.5] <- "Schizophrenia"
train_informed_s$Predictions3[train_informed_s$PredictionsPerc3 <= 0.5] <- "Control"


train_informed_s <- train_informed_s %>% 
  mutate(
    Group = as.factor(Group), 
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2),
    Predictions3 = as.factor(Predictions3)
  )

train_informed_s

# generate average predictions 
### informed
##for the test dataset

# fixed effect
test_informed_s$PredictionsPerc1 <- predict(Variables_fit1s,newdata = test_informed_s,allow_new_levels = T)[, 1]
test_informed_s$Predictions1[test_informed_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
test_informed_s$Predictions1[test_informed_s$PredictionsPerc1 <= 0.5] <- "Control"

#vary intercept
test_informed_s$PredictionsPerc2 <- predict(Variables_fit2s,newdata = test_informed_s,allow_new_levels = T)[, 1]
test_informed_s$Predictions2[test_informed_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
test_informed_s$Predictions2[test_informed_s$PredictionsPerc2 <= 0.5] <- "Control"

#vary slopes
test_informed_s$PredictionsPerc3 <- predict(Variables_fit3s,newdata = test_informed_s,allow_new_levels = T)[, 1]
test_informed_s$Predictions3[test_informed_s$PredictionsPerc3 > 0.5] <- "Schizophrenia"
test_informed_s$Predictions3[test_informed_s$PredictionsPerc3 <= 0.5] <- "Control"


test_informed_s <- test_informed_s %>% 
  mutate(
    Group = as.factor(Group), 
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2),
    Predictions3 = as.factor(Predictions3)
  )

test_informed_s
```

```{r}
# generate average predictions 
# skeptic for the trained dataset
# fixed effect
train_skeptic_s$PredictionsPerc1 <- predict(Variables_fit1i)[, 1]
train_skeptic_s$Predictions1[train_skeptic_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
train_skeptic_s$Predictions1[train_skeptic_s$PredictionsPerc1 <= 0.5] <- "Control"

#vary intercept
train_skeptic_s$PredictionsPerc2 <- predict(Variables_fit2i)[, 1]
train_skeptic_s$Predictions2[train_skeptic_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
train_skeptic_s$Predictions2[train_skeptic_s$PredictionsPerc2 <= 0.5] <- "Control"

#vary slopes
train_skeptic_s$PredictionsPerc3 <- predict(Variables_fit3i)[, 1]
train_skeptic_s$Predictions3[train_skeptic_s$PredictionsPerc3 > 0.5] <- "Schizophrenia"
train_skeptic_s$Predictions3[train_skeptic_s$PredictionsPerc3 <= 0.5] <- "Control"


train_skeptic_s <- train_skeptic_s %>% 
  mutate(
    Group = as.factor(Group), 
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2),
    Predictions3 = as.factor(Predictions3)
  )

train_skeptic_s

# generate average predictions 
### skeptic
##for the trained dataset

# fixed effect
test_skeptic_s$PredictionsPerc1 <- predict(Variables_fit1s,newdata = test_skeptic_s,allow_new_levels = T)[, 1]
test_skeptic_s$Predictions1[test_skeptic_s$PredictionsPerc1 > 0.5] <- "Schizophrenia"
test_skeptic_s$Predictions1[test_skeptic_s$PredictionsPerc1 <= 0.5] <- "Control"

#vary intercept
test_skeptic_s$PredictionsPerc2 <- predict(Variables_fit2s,newdata = test_skeptic_s,allow_new_levels = T)[, 1]
test_skeptic_s$Predictions2[test_skeptic_s$PredictionsPerc2 > 0.5] <- "Schizophrenia"
test_skeptic_s$Predictions2[test_skeptic_s$PredictionsPerc2 <= 0.5] <- "Control"

#vary slopes
test_skeptic_s$PredictionsPerc3 <- predict(Variables_fit3s,newdata = test_skeptic_s,allow_new_levels = T)[, 1]
test_skeptic_s$Predictions3[test_skeptic_s$PredictionsPerc3 > 0.5] <- "Schizophrenia"
test_skeptic_s$Predictions3[test_skeptic_s$PredictionsPerc3 <= 0.5] <- "Control"


test_skeptic_s <- test_skeptic_s %>% 
  mutate(
    Group = as.factor(Group), 
    Predictions1 = as.factor(Predictions1),
    Predictions2 = as.factor(Predictions2),
    Predictions3 = as.factor(Predictions3)
  )

test_skeptic_s
```

### Assesing average performance
```{r}
# Assessing average performance for informed
conf_mat(
  train_informed_s,
  truth = Group,
  estimate = Predictions1,
  dnn = c("Prediction","Truth")
)

metrics(train_informed_s,
        truth = Group,
        estimate = Predictions1) %>% 
  knitr::kable()

conf_mat(
  train_informed_s,
  truth = Group,
  estimate = Predictions2,
  dnn = c("Prediction","Truth")
)

metrics(train_informed_s,
        truth = Group,
        estimate = Predictions2) %>% 
  knitr::kable()

conf_mat(
  train_informed_s,
  truth = Group,
  estimate = Predictions3,
  dnn = c("Prediction","Truth")
)

metrics(train_informed_s,
        truth = Group,
        estimate = Predictions3) %>% 
  knitr::kable()

## For test
metrics(test_informed_s,
        truth = Group,
        estimate = Predictions1) %>% 
  knitr::kable()

metrics(test_informed_s,
        truth = Group,
        estimate = Predictions2) %>% 
  knitr::kable()

metrics(test_informed_s,
        truth = Group,
        estimate = Predictions3) %>% 
  knitr::kable()


# Assessing average performance for skeptic
conf_mat(
  train_skeptic_s,
  truth = Group,
  estimate = Predictions1,
  dnn = c("Prediction","Truth")
)

metrics(train_skeptic_s,
        truth = Group,
        estimate = Predictions1) %>% 
  knitr::kable()

conf_mat(
  train_skeptic_s,
  truth = Group,
  estimate = Predictions2,
  dnn = c("Prediction","Truth")
)

metrics(train_skeptic_s,
        truth = Group,
        estimate = Predictions2) %>% 
  knitr::kable()

conf_mat(
  train_skeptic_s,
  truth = Group,
  estimate = Predictions3,
  dnn = c("Prediction","Truth")
)

metrics(train_skeptic_s,
        truth = Group,
        estimate = Predictions3) %>% 
  knitr::kable()

## For test
metrics(train_skeptic_s,
        truth = Group,
        estimate = Predictions1) %>% 
  knitr::kable()

metrics(train_skeptic_s,
        truth = Group,
        estimate = Predictions2) %>% 
  knitr::kable()

metrics(train_skeptic_s,
        truth = Group,
        estimate = Predictions3) %>% 
  knitr::kable()
```
### Calculating uncertainity 
```{r}
## Now with uncertainty
pacman::p_load(tidyverse)

PerformanceProb <- tibble(expand_grid(
  Sample = seq(5000),
  Model = c("FixedEffects","VaryingIntercept","VariyingSlope"),
  Setup = c("informed","skeptic"),
  Type = c("training","test"))
)
# posterior predict for training informed and skeptic

train1i <- inv_logit_scaled(posterior_linpred(Variables_fit1i,summary = F))
train2i <- inv_logit_scaled(posterior_linpred(Variables_fit2i,summary = F))
train3i <- inv_logit_scaled(posterior_linpred(Variables_fit3i,summary = F))

train1s <- inv_logit_scaled(posterior_linpred(Variables_fit1s,summary = F))
train2s <- inv_logit_scaled(posterior_linpred(Variables_fit2s,summary = F))
train3s <- inv_logit_scaled(posterior_linpred(Variables_fit3s,summary = F))

# same but for test
test1i <- inv_logit_scaled(posterior_linpred(Variables_fit1i,summary = F, newdata = test_informed_s,allow_new_levels = T))
test2i <- inv_logit_scaled(posterior_linpred(Variables_fit2i,summary = F, newdata = test_informed_s,allow_new_levels = T))
test3i <- inv_logit_scaled(posterior_linpred(Variables_fit3i,summary = F, newdata = test_informed_s,allow_new_levels = T))

test1s <- inv_logit_scaled(posterior_linpred(Variables_fit1s,summary = F, newdata = test_skeptic_s,allow_new_levels = T))
test2s <- inv_logit_scaled(posterior_linpred(Variables_fit2s,summary = F, newdata = test_skeptic_s,allow_new_levels = T))
test3s <- inv_logit_scaled(posterior_linpred(Variables_fit3s,summary = F, newdata = test_skeptic_s,allow_new_levels = T))

for (i in seq(5000)){
  # inserting the predictions into the df for informed and skeptic
  train_informed_s$Predictions1 <- as.factor(ifelse(train1i[i,] > 0.5, "Schizophrenia","Control"))
  train_informed_s$Predictions2 <- as.factor(ifelse(train2i[i,] > 0.5, "Schizophrenia","Control"))
  train_informed_s$Predictions3 <- as.factor(ifelse(train3i[i,] > 0.5, "Schizophrenia","Control"))

  test_informed_s$Predictions1 <- as.factor(ifelse(test1i[i,] > 0.5, "Schizophrenia","Control"))
  test_informed_s$Predictions2 <- as.factor(ifelse(test2i[i,] > 0.5, "Schizophrenia","Control"))
  test_informed_s$Predictions3 <- as.factor(ifelse(test3i[i,] > 0.5, "Schizophrenia","Control"))
  
  train_skeptic_s$Predictions1 <- as.factor(ifelse(train1s[i,] > 0.5, "Schizophrenia","Control"))
  train_skeptic_s$Predictions2 <- as.factor(ifelse(train2s[i,] > 0.5, "Schizophrenia","Control"))
  train_skeptic_s$Predictions3 <- as.factor(ifelse(train3s[i,] > 0.5, "Schizophrenia","Control"))

  test_skeptic_s$Predictions1 <- as.factor(ifelse(test1s[i,] > 0.5, "Schizophrenia","Control"))
  test_skeptic_s$Predictions2 <- as.factor(ifelse(test2s[i,] > 0.5, "Schizophrenia","Control"))
  test_skeptic_s$Predictions3 <- as.factor(ifelse(test3s[i,] > 0.5, "Schizophrenia","Control"))
  
  # put it into the performance prob df
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "FixedEffects" & PerformanceProb$Setup == "informed" & PerformanceProb$Type == "training"] <- accuracy(train_informed_s,truth = Group, estimate = Predictions1)[,".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" & PerformanceProb$Setup == "informed" & PerformanceProb$Type == "training"] <- accuracy(train_informed_s,truth = Group, estimate = Predictions2)[,".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VariyingSlope" & PerformanceProb$Setup == "informed" & PerformanceProb$Type == "training"] <- accuracy(train_informed_s,truth = Group, estimate = Predictions3)[,".estimate"]
  
  
   PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "FixedEffects" & PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "training"] <- accuracy(train_skeptic_s,truth = Group, estimate = Predictions1)[,".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" & PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "training"] <- accuracy(train_skeptic_s,truth = Group, estimate = Predictions2)[,".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VariyingSlope" & PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "training"] <- accuracy(train_skeptic_s,truth = Group, estimate = Predictions3)[,".estimate"]
  
  # the same for tests
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "FixedEffects" & PerformanceProb$Setup == "informed" & PerformanceProb$Type == "test"] <- accuracy(test_informed_s,truth = Group, estimate = Predictions1)[,".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" & PerformanceProb$Setup == "informed" & PerformanceProb$Type == "test"] <- accuracy(test_informed_s,truth = Group, estimate = Predictions2)[,".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VariyingSlope" & PerformanceProb$Setup == "informed" & PerformanceProb$Type == "test"] <- accuracy(test_informed_s,truth = Group, estimate = Predictions3)[,".estimate"]
  
  
   PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "FixedEffects" & PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "test"] <- accuracy(test_skeptic_s,truth = Group, estimate = Predictions1)[,".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VaryingIntercept" & PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "test"] <- accuracy(test_skeptic_s,truth = Group, estimate = Predictions2)[,".estimate"]
  
  PerformanceProb$Accuracy[PerformanceProb$Sample == i & PerformanceProb$Model == "VariyingSlope" & PerformanceProb$Setup == "skeptic" & PerformanceProb$Type == "test"] <- accuracy(test_skeptic_s,truth = Group, estimate = Predictions3)[,".estimate"]
  
}

PerformanceProb$Accuracy <- as.numeric(PerformanceProb$Accuracy)
```
### Plot of uncertainity of performance probablity of the different models
```{r}
ggplot(PerformanceProb) +
  aes(x = Model, y = Accuracy, colour = Type) +
  facet_wrap(~Setup) +
  geom_point(shape=16,position = position_dodge(width = 0.5)) +
  scale_color_hue(direction = 1) +
  theme_grey()
```
```{r}

```


### Feature importance
```{r}
pacman::p_load(DALEX,DALEXtra,kernlab,randomForest,xgboost,knitr,dotwhisker)

d_inf <- train_informed_s %>% 
  mutate(Trial = NULL,Preds = NULL,
         Predictions1 = NULL,PredictionsPerc1 = NULL,
         Predictions2 = NULL,PredictionsPerc2 = NULL,
         Predictions3 = NULL,PredictionsPerc3 = NULL)

LogisticRegression_inf <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm") %>% 
  fit(Group~1+Pitch_mode+Pitch_vari+Speech_rate+Prop_spokenT+Pause_n+Pause_len+ Noise1+Noise2+Noise3+Noise4+(Pitch_mode+Pitch_vari+Speech_rate+Prop_spokenT+Pause_n+Pause_len+Noise1+Noise2+Noise3+Noise4|ID),data = d_inf)

explainer_lm <- 
  explain_tidymodels(
    LogisticRegression_inf,
    data = train_informed_s,
    y = as.numeric(train_informed_s$Group) - 1,
    label = "logReg",
    verbose = FALSE
  )

lm_plot <- explainer_lm %>% 
  model_parts() %>% 
  plot(show_boxplots = FALSE) + 
  ggtitle("Feature Importance","")
```

```{r}
RandomForest_inf <- rand_forest() %>% 
  set_mode("classification") %>% 
  set_engine("randomForest") %>% 
  fit(Group~1+Pitch_mode+Pitch_vari+Speech_rate+Prop_spokenT+Pause_n+Pause_len+ Noise1+Noise2+Noise3+Noise4+(Pitch_mode+Pitch_vari+Speech_rate+Prop_spokenT+Pause_n+Pause_len+Noise1+Noise2+Noise3+Noise4|ID),data = d_inf)

explainer_rf <- 
  explain_tidymodels(
    RandomForest_inf,
    data = train_informed_s,
    y = as.numeric(train_informed_s$Group) - 1,
    label = "random forest",
    verbose = FALSE
  )

rf_plot <- explainer_rf %>% 
  model_parts() %>% 
  plot(show_boxplots = FALSE) +
  ggtitle("Feature Importance","")
```

```{r}
grid.arrange(lm_plot,rf_plot,nrow=1)
```
```{r}
model_profile_lm1 <- model_profile(explainer_lm,type="partial",
                                   variables = c("Pitch_mode","Pitch_vari","Speech_rate","Prop_spokenT","Pause_n","Pause_len", "Noise1","Noise2","Noise3","Noise4"))

plot(model_profile_lm1,variables =c("Pitch_mode","Pitch_vari","Speech_rate","Prop_spokenT","Pause_n","Pause_len", "Noise1","Noise2","Noise3","Noise4"))+
  ggtitle("Partial dependence profile","")

model_profile_lm2 <- model_profile(explainer_rf,type="partial",
                                   variables = c("Pitch_mode","Pitch_vari","Speech_rate","Prop_spokenT","Pause_n","Pause_len", "Noise1","Noise2","Noise3","Noise4"))

plot(model_profile_lm2,variables =c("Pitch_mode","Pitch_vari","Speech_rate","Prop_spokenT","Pause_n","Pause_len", "Noise1","Noise2","Noise3","Noise4"))+
  ggtitle("Partial dependence profile","")

```


## Part III - Applying the ML pipeline to empirical data

Download the empirical dataset from brightspace and apply your ML pipeline to the new data, adjusting where needed.

Warning: in the simulated dataset we only had 10 features, now you have many more! Such is the life of the ML practitioner. Consider the impact a higher number of features will have on your ML inference, and decide whether you need to cut down the number of features before running the pipeline (or alternatively expand the pipeline to add feature selection).

### Reading in the data
```{r}
EDf <- read_csv("Ass3_empiricalData1.csv")

head(EDf)
```

### Summarizing
```{r}
EDf%>% 
  subset(Trial=="T1") %>% 
  filter(Gender=="M"|Gender=="F",Diagnosis=="CT"|Diagnosis=="SCZ") %>% 
  group_by(Gender,Diagnosis) %>% 
  summarise(n=length(unique(NewID)))
```

### Creating a data budget
```{r}
pacman::p_load(groupdata2)
# Creating a data budget
EDf <- subset(EDf,select=c(-Gender,-PatID,-Trial,-Corpus,-Language))

EDf$NewID <- factor(EDf$NewID,levels=unique(EDf$NewID))
EDf$Diagnosis <- as.factor(EDf$Diagnosis)

head(EDf)
```

```{r}
set.seed(1373)
partitions <- partition(EDf,p = 0.2,id_col="NewID",cat_col = "Diagnosis")

trainDf <- partitions[[2]]
testDf <- partitions[[1]]

rec_EDf <- trainDf %>% 
  recipe(Diagnosis ~ . ) %>% 
  update_role(NewID, Diagnosis, new_role = "NewID") %>%
  step_scale(all_numeric()) %>% 
  step_center(all_numeric()) %>%
  step_zv(all_predictors()) %>%
  step_corr(all_predictors(), threshold = 0.8) %>% 
  prep(training=trainDf,retain=TRUE)

# Apply recipe to train and test
trainEDf <- juice(rec_EDf)
testEDf <- bake(rec_EDf,new_data = testDf)

trainEDf$NewID <- as.integer(trainEDf$NewID)
testEDf$NewID <- as.integer(testEDf$NewID)
```

### Feature selection
```{r}
pacman::p_load(DALEX,DALEXtra,kernlab,randomForest,xgboost,knitr,dotwhisker,ggcorrplot,gam)
d_edf <- trainEDf %>% 
  mutate(Trial = NULL,PatID = NULL,Language = NULL, Gender = NULL, Corpus = NULL)


LogisticRegression_inf_edf <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm") %>% 
  fit(Diagnosis ~ 1 + . + (1|NewID),data = d_edf)

explainer_lm_edf <- 
  explain_tidymodels(
    LogisticRegression_inf_edf,
    data = d_edf,
    y = as.numeric(d_edf$Diagnosis) - 1,
    label = "logReg",
    verbose = FALSE
  )

fi_plot <- explainer_lm_edf %>% 
  model_parts() %>% 
  plot(show_boxplots = FALSE) + 
  ggtitle("Feature Importance","")
```

```{r}
fi_plot
```

```{r}
fs_model <- explainer_lm_edf %>% 
  model_parts()


fs_df <- as.data.frame(fs_model)
fs_df
```


```{r}
print(explainer_lm_edf)
```

```{r}
fs_df %>%
  arrange(desc(dropout_loss))
```



Choose:
Clarity_Mean + MCEP1_Median + MCEP1_SD + NewID + peakSlope_SD + MCEP8_Mean + HarmonicProductSpectrum_Mean + F0_Mean_Praat +  MCEP6_Mean + HMPDD0_MAD + MCEP13_Mean + MCEP5_Mean + Rd_conf_Median + peakSlope_IQR + MCEP0_SD + HMPDD7_Median + F1_IQR + MCEP12_Median + Srh2_SD + HMPDD6_Mean + MeanPauseDur_Cova + Intensity_SD_Praat + HMPDD0_SD + TurnDuration_Cova + Rd_Mean


### Fitting a classifcation algorithm on the traning data 
```{r}
library(brms)
# fitting and assess a classification algorithm on the training data (e.g. Bayesian multilevel logistic regression)

# Formula
FormulaDf_0 <- bf(Diagnosis~1 + Clarity_Mean + MCEP1_Median + MCEP1_SD + NewID + peakSlope_SD + MCEP8_Mean + HarmonicProductSpectrum_Mean + F0_Mean_Praat +  MCEP6_Mean + HMPDD0_MAD + (1|NewID))

FormulaDf_1 <- bf(Diagnosis~1 + Clarity_Mean + MCEP1_Median + MCEP1_SD + NewID + peakSlope_SD + MCEP8_Mean + HarmonicProductSpectrum_Mean + F0_Mean_Praat +  MCEP6_Mean + HMPDD0_MAD + MCEP13_Mean + MCEP5_Mean + Rd_conf_Median + peakSlope_IQR + MCEP0_SD + HMPDD7_Median + F1_IQR + MCEP12_Median + Srh2_SD + HMPDD6_Mean + MeanPauseDur_Cova + Intensity_SD_Praat + HMPDD0_SD + TurnDuration_Cova + Rd_Mean + (1|NewID))

FormulaDf_2 <- bf(Diagnosis~1 + Clarity_Mean + MCEP1_Median + MCEP1_SD + NewID + peakSlope_SD + MCEP8_Mean + HarmonicProductSpectrum_Mean + F0_Mean_Praat +  MCEP6_Mean + HMPDD0_MAD + MCEP13_Mean + MCEP5_Mean + Rd_conf_Median + peakSlope_IQR + MCEP0_SD + HMPDD7_Median + F1_IQR + MCEP12_Median + Srh2_SD + HMPDD6_Mean + MeanPauseDur_Cova + Intensity_SD_Praat + HMPDD0_SD + TurnDuration_Cova + Rd_Mean + Intensity_Mean_Praat + MCEP4_Median + MCEP15_Mean + MCEP20_Mean + PauseNumMin_Praat + (1|NewID))

# Prior
EDf_p <- c(
  brms::prior(normal(0,1),class=Intercept),
  brms::prior(normal(0,1),class=b),
  brms::prior(normal(0,.1),class=sd)
)

#get_prior(FormulaDf_1,family=bernoulli,data=EDf)

EDf_m_0 <- brm(
  FormulaDf_1,
  trainEDf,
  family = bernoulli,
  prior = EDf_p,
  sample_prior = "only",
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  threads = threading(4)
)

EDf_m_1 <- brm(
  FormulaDf_1,
  trainEDf,
  family = bernoulli,
  prior = EDf_p,
  sample_prior = "only",
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  threads = threading(4)
)

EDf_m_2 <- brm(
  FormulaDf_1,
  trainEDf,
  family = bernoulli,
  prior = EDf_p,
  sample_prior = "only",
  backend = "cmdstanr",
  chains = 2,
  cores = 2,
  threads = threading(4)
)

pp_check(EDf_m_0, ndraws = 100) + labs(title="Prior predictive checks emprical data, model with 10 variables")

pp_check(EDf_m_1, ndraws = 100) + labs(title="Prior predictive checks emprical data, model with 25 variables")

pp_check(EDf_m_2, ndraws = 100) + labs(title="Prior predictive checks emprical data, model with 30 variables")
```


### Doing the fit, posterior
```{r}
EDf_post_0 <-
  brm(
    FormulaDf_0,
    trainEDf,
    save_pars = save_pars(all = TRUE),
    family = bernoulli,
    prior = EDf_p,
    file = "EDf_post",
    sample_prior = T,
    iter = 5000, 
    warmup = 2000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(4)
  )

EDf_post_1 <-
  brm(
    FormulaDf_1,
    trainEDf,
    save_pars = save_pars(all = TRUE),
    family = bernoulli,
    prior = EDf_p,
    file = "EDf_post",
    sample_prior = T,
    iter = 5000, 
    warmup = 2000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(4)
  )

EDf_post_2 <-
  brm(
    FormulaDf_2,
    trainEDf,
    save_pars = save_pars(all = TRUE),
    family = bernoulli,
    prior = EDf_p,
    file = "EDf_post",
    sample_prior = T,
    iter = 5000, 
    warmup = 2000,
    cores = 2,
    chains = 2,
    backend = "cmdstanr",
    threads = threading(4)
  )
```


### Plots drawing from posterior, posterior update plots
```{r}
posteriorEDf_0 <- as_draws_df(EDf_post_0)
posteriorEDf_1 <- as_draws_df(EDf_post_1)
posteriorEDf_2 <- as_draws_df(EDf_post_2)

# Posterior drawings
ggplot(posteriorEDf) +
  geom_density(aes(b_Intercept),fill="blue", alpha=0.3) +
  geom_density(aes(prior_Intercept), fill="red", alpha=0.3)+
  labs(title = "Posterior intercept")+
  theme_minimal()

ggplot(posteriorEDf) +
  geom_density(aes(prior_b), fill="red") +
  geom_density(aes(b_Clarity_Mean), fill="orange", alpha=0.3) +
  geom_density(aes(b_MCEP1_Median), fill="salmon", alpha=0.3) +
  geom_density(aes(b_MCEP1_SD), fill="green", alpha=0.3) +
  #geom_density(aes(b_NewID), fill="blue", alpha=0.3) +
  geom_density(aes(b_peakSlope_SD), fill="purple", alpha=0.3) +
  geom_density(aes(b_MCEP8_Mean), fill="pink", alpha=0.3) +
  geom_density(aes(b_HarmonicProductSpectrum_Mean), fill="lavender", alpha=0.3) +
  geom_density(aes(b_F0_Mean_Praat), fill="gray", alpha=0.3) +
  geom_density(aes(b_MCEP6_Mean), fill="brown", alpha=0.3) +
  geom_density(aes(b_HMPDD0_MAD), fill="maroon", alpha=0.3) +
  geom_density(aes(b_MCEP13_Mean), fill="Turquoise", alpha=0.3) +
  geom_density(aes(b_MCEP5_Mean), fill="turquoise", alpha=0.3) +
  geom_density(aes(b_Rd_conf_Median), fill="sky blue", alpha=0.3) +
  geom_density(aes(b_peakSlope_IQR), fill="black", alpha=0.3) +
  geom_density(aes(b_MCEP0_SD), fill="chartreuse", alpha=0.3) +
  geom_density(aes(b_HMPDD7_Median), fill="cyan", alpha=0.3) +
  geom_density(aes(b_F1_IQR), fill="coral", alpha=0.3) +
  geom_density(aes(b_Srh2_SD), fill="#123997", alpha=0.3) +
  geom_density(aes(b_HMPDD6_Mean), fill="navyblue", alpha=0.3) +
  geom_density(aes(b_Intensity_SD_Praat), fill="magenta", alpha=0.3) +
  geom_density(aes(b_HMPDD0_SD), fill="white", alpha=0.3) +
  geom_density(aes(b_TurnDuration_Cova), fill="#123789", alpha=0.3) +
  geom_density(aes(b_Rd_Mean), fill="#129203", alpha=0.3) +
  xlim(-3,3)+
  labs(title = "Posterior b, for variables") +
  theme_minimal()


ggplot(posteriorEDf) +
  geom_density(aes(prior_sd_NewID), fill="red") +
  geom_density(aes(sd_NewID__Intercept), fill="blue", alpha=0.3) +
  labs(title = "Posterior b, for sd") +
  theme_minimal()


ggplot(posteriorEDf) +
  geom_density(aes(y = ..density..), alpha = 0.5, fill = "transparent") +
  labs(title = "Densities for Multiple Variables",
       x = "Variable",
       y = "Density")
```


### Generating average predictions
```{r}
# generate average predictions 
trainEDf$PredictionsPerc <-  predict(EDf_pos)[, 1]
trainEDf$Predictions[trainEDf$PredictionsPerc > 0.5] <- "SCZ"
trainEDf$Predictions[trainEDf$PredictionsPerc <= 0.5] <- "CT"

trainEDf <- trainEDf %>% 
  mutate(
    Diagnosis = as.factor(Diagnosis), 
    Predictions = as.factor(Predictions),
  )

trainEDf

testEDf$PredictionsPerc <- predict(EDf_pos,newdata = testEDf,allow_new_levels = T)[, 1]
testEDf$Predictions[testEDf$PredictionsPerc > 0.5] <- "SCZ"
testEDf$Predictions[testEDf$PredictionsPerc <= 0.5] <- "CT"


testEDf <- testEDf %>% 
  mutate(
    Diagnosis = as.factor(Diagnosis), 
    Predictions = as.factor(Predictions),
  )

testEDf
```

### Assesing average performance
```{r}
# Assessing average performance for informed
conf_mat(
  trainEDf,
  truth = Diagnosis,
  estimate = Predictions,
  dnn = c("Prediction","Truth")
)

metrics(trainEDf,
        truth = Diagnosis,
        estimate = Predictions) %>% 
  knitr::kable()

## For test
metrics(testEDf,
        truth = Diagnosis,
        estimate = Predictions) %>% 
  knitr::kable()
```

### Calculating uncertainity 
```{r}
## Now with uncertainty
pacman::p_load(tidyverse)

PerformanceProbEDf <- tibble(expand_grid(
  Sample = seq(5000),
  Type = c("training","test"))
)
# posterior predict for training 
train_EDf <- inv_logit_scaled(posterior_linpred(EDf_post,summary = F))

# same but for test
test_EDf <- inv_logit_scaled(posterior_linpred(EDf_post,summary = F, newdata = testEDf,allow_new_levels = T))

for (i in seq(5000)){
  # inserting the predictions into the df for informed and skeptic
  trainEDf$Predictions <- as.factor(ifelse(train_EDf[i,] > 0.5, "SCZ","CT"))
  testEDf$Predictions <- as.factor(ifelse(test_EDf[i,] > 0.5, "SCZ","CT"))
  
  # put it into the performance prob df
  PerformanceProbEDf$Accuracy[PerformanceProbEDf$Sample == i & PerformanceProbEDf$Type == "training"] <- accuracy(trainEDf,truth = Diagnosis, estimate = Predictions)[,".estimate"]
  
  # the same for tests
  PerformanceProbEDf$Accuracy[PerformanceProbEDf$Sample == i & PerformanceProbEDf$Type == "test"] <- accuracy(testEDf,truth = Diagnosis, estimate = Predictions)[,".estimate"]
}

PerformanceProbEDf$Accuracy <- as.numeric(PerformanceProbEDf$Accuracy)
```
### Plot of uncertainity of performance probablity
```{r}
ggplot(PerformanceProbEDf) +
  aes(x = Type, y = Accuracy) +
  geom_point(shape=16,position = position_dodge(width = 0.5)) +
  scale_color_hue(direction = 1) +
  theme_grey()
```




